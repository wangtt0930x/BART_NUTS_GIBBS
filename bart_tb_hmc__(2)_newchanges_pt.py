# -*- coding: utf-8 -*-
"""BART_TB_HMC_ (2)_NEWCHANGES_PT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eIw7COdfU1SuNEnehV_gRzrOteKekdOM

## Import
"""

from operator import gt, le
from typing import Any, List, Optional, Union
from abc import abstractmethod, ABC
from copy import deepcopy
from collections import defaultdict
from tqdm import tqdm
from sklearn.ensemble import GradientBoostingRegressor
from joblib import Parallel, delayed
from sklearn.base import RegressorMixin, BaseEstimator
from sklearn.metrics import mean_squared_error

import math
import numpy as np
import pandas as pd
from scipy.special import gammaln
from mpmath import mp

"""## Simple Simulated Data"""

np.random.seed(42)
# Number of samples
n = 50
# Generate features
X1 = np.linspace(0, 10, n)
X2 = np.linspace(5, 15, n)
X_irrelevant = np.random.randn(n, 3)  # Irrelevant features
# Combine features
X = np.column_stack([X1, X2, X_irrelevant])

# Generate response variable with noise
noise = np.random.normal(0, 0.5, n)
y = X1 + X2 + noise

# Initialize BART model parameters
num_trees = 20
alpha = 0.95
beta = 2.0
burn_in = 10
num_samples = 200
alpha_prior = 2
beta_prior = 2

"""## Causal Inference Data"""

import pandas as pd
practice_df = pd.read_csv('acic_practice_0001.csv')
practice_year_df = pd.read_csv('acic_practice_year_0001.csv')
merged_df = pd.merge(practice_df, practice_year_df, on='id.practice')
merged_df.head()

X = merged_df[['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9']]
X = pd.get_dummies(X, columns=['X2', 'X4'], drop_first=True)
Z = merged_df['Z'].values.astype(float)
Y = merged_df['Y'].values.astype(float)
X_treated = np.hstack([X, np.ones((X.shape[0], 1))])
X_control = np.hstack([X, np.zeros((X.shape[0], 1))])
X_full = np.hstack([X, Z.reshape(-1, 1)])
X.shape, Z.shape, Y.shape, X_treated.shape, X_control.shape, X_full.shape

"""## Tree Structure

This class helps in defining the conditions for splitting the data based on a specific variable during the tree-building process. It contains methods for adding new conditions and updating existing ones.
"""

class CombinedVariableCondition(object):

    def __init__(self, splitting_variable: int, min_value: float, max_value: float):
        self.splitting_variable = splitting_variable
        self.min_value, self.max_value = min_value, max_value

    def add_condition(self, split_condition) -> 'CombinedVariableCondition':
        if self.splitting_variable != split_condition.splitting_variable:
            return self
        if split_condition.operator == gt and split_condition.splitting_value > self.min_value:
            return CombinedVariableCondition(self.splitting_variable, split_condition.splitting_value, self.max_value)
        elif split_condition.operator == le and split_condition.splitting_value < self.max_value:
            return CombinedVariableCondition(self.splitting_variable, self.min_value, split_condition.splitting_value)
        else:
            return self

"""CombinedCondition Class: This class extends the concept of CombinedVariableCondition to handle multiple variables simultaneously. It keeps track of conditions applied to each variable, and it can combine these conditions to filter data points effectively."""

class CombinedCondition(object):

    def __init__(self, variables: List[int], conditions):
        self.variables = {v: CombinedVariableCondition(v, -np.inf, np.inf) for v in variables}
        self.conditions = conditions
        for condition in conditions:
            self.variables[condition.splitting_variable] = self.variables[condition.splitting_variable].add_condition(condition)
        if len(conditions) > 0:
            self.splitting_variable = conditions[-1].splitting_variable
        else:
            self.splitting_variable = None

    def condition(self, X: np.ndarray) -> np.ndarray:
        c = np.array([True] * len(X))
        for variable in self.variables.keys():
            c = c & (X[:, variable] > self.variables[variable].min_value) & (X[:, variable] <= self.variables[variable].max_value)
        return c

    def __add__(self, other):
        return CombinedCondition(list(self.variables.keys()), self.conditions + [other])

    def most_recent_split_condition(self):
        if len(self.conditions) == 0:
            return None
        else:
            return self.conditions[-1]

"""Split Class: This class represents a specific data split in the decision tree. It holds a CombinedCondition object and a Data object. This class is essential for evaluating the conditions and maintaining the data at each node of the decision tree."""

class Split:
    """
    The Split class represents the conditioned data at any point in the decision tree
    It contains the logic for:

     - Maintaining a record of which rows of the covariate matrix are in the split
     - Being able to easily access a `Data` object with the relevant rows
     - Applying `SplitConditions` to further break up the data
    """

    def __init__(self, data, combined_condition=None):
        self._data = data
        if combined_condition is None:
            combined_condition = CombinedCondition(self._data.X.variables, [])
        self._combined_condition = combined_condition

    @property
    def data(self):
        return self._data

    def combined_condition(self):
        return self._combined_condition

    def condition(self, X: np.ndarray=None) -> np.array:
        if X is None:
            return ~self._data.mask
        else:
            return self.out_of_sample_condition(X)

    def out_of_sample_condition(self, X: np.ndarray) -> np.ndarray:
        return self._combined_condition.condition(X)

    def out_of_sample_conditioner(self):
        return self._combined_condition

    def __add__(self, other):

        return Split(self._data + other,
                     self._combined_condition + other)

    def most_recent_split_condition(self):
        return self._combined_condition.most_recent_split_condition()

"""Tree Class: This class encapsulates a single decision tree in the Bayesian Additive Regression Trees (BART) model. It maintains a list of nodes, which can be leaf nodes or decision nodes. The class provides functionalities for updating target values (y) in all nodes, generating predictions, and adding or removing nodes."""

class Tree:
    """
    An encapsulation of the structure of a single decision tree
    Contains no logic, but keeps track of 4 different kinds of nodes within the tree:
      - leaf nodes
      - decision nodes
      - splittable leaf nodes
      - prunable decision nodes

    Parameters
    ----------
    nodes: List[Node]
        All nodes contained in the tree, i.e. decision and leaf nodes
    """

    def __init__(self, nodes):
        self._nodes = nodes
        self.cache_up_to_date = False
        self._prediction = None

    @property
    def nodes(self):
        """
        List of all nodes contained in the tree
        """
        return self._nodes

    @property
    def leaf_nodes(self):
        """
        List of all of the leaf nodes in the tree
        """
        return [x for x in self._nodes if type(x) == LeafNode]

    @property
    def splittable_leaf_nodes(self):
        """
        List of all leaf nodes in the tree which can be split in a non-degenerate way
        i.e. not all rows of the covariate matrix are duplicates
        """
        return [x for x in self.leaf_nodes if x.is_splittable()]

    @property
    def decision_nodes(self):
        """
        List of decision nodes in the tree.
        Decision nodes are internal split nodes, i.e. not leaf nodes
        """
        return [x for x in self._nodes if type(x) == DecisionNode]

    @property
    def prunable_decision_nodes(self):
        """
        List of decision nodes in the tree that are suitable for pruning
        In particular, decision nodes that have two leaf node children
        """
        return [x for x in self.decision_nodes if x.is_prunable()]

    def update_y(self, y: np.ndarray) -> None:
        """
        Update the cached value of the target array in all nodes
        Used to pass in the residuals from the sum of all of the other trees
        """
        self.cache_up_to_date = False
        for node in self.nodes:
            node.update_y(y)

    def predict(self, X: np.ndarray=None) -> np.ndarray:
        """
        Generate a set of predictions with the same dimensionality as the target array
        Note that the prediction is from one tree, so represents only (1 / number_of_trees) of the target
        """
        if X is not None:
            return self._out_of_sample_predict(X)

        if self.cache_up_to_date:
            return self._prediction
        for leaf in self.leaf_nodes:
            if self._prediction is None:
                self._prediction = np.zeros(self.nodes[0].data.X.n_obsv)
            self._prediction[leaf.split.condition()] = leaf.predict()
        self.cache_up_to_date = True
        return self._prediction

    def _out_of_sample_predict(self, X) -> np.ndarray:
        """
        Prediction for a covariate matrix not used for training

        Note that this is quite slow

        Parameters
        ----------
        X: pd.DataFrame
            Covariates to predict for
        Returns
        -------
        np.ndarray
        """
        prediction = np.array([0.] * len(X))
        for leaf in self.leaf_nodes:
            prediction[leaf.split.condition(X)] = leaf.predict()
        return prediction

    def remove_node(self, node) -> None:
        """
        Remove a single node from the tree
        Note that this is non-recursive, only drops the node and not any children
        """
        self._nodes.remove(node)

    def add_node(self, node) -> None:
        """
        Add a node to the tree
        Note that this is non-recursive, only adds the node and not any children
        """
        self._nodes.append(node)

"""TreeMutation Class: This class represents a modification that can be applied to a tree structure. It keeps track of what kind of change is to be made ("grow" or "prune"), the node to be altered, and what it should be changed to. This class could be particularly useful in the MCMC sampling process where trees are altered iteratively."""

class TreeMutation(object):
    """
    An encapsulation of a change to be made to the tree.
    Constructed of three components
      - the node to be changed
      - what it should be changed to
      - a string name of the kind of change (normally grow or prune)
    """

    def __init__(self, kind: str, existing_node, updated_node):
        self.kind = kind
        self.existing_node = existing_node
        self.updated_node = updated_node

    def __str__(self):
        return "{} - {} => {}".format(self.kind, self.existing_node, self.updated_node)

"""PruneMutation Class: This is a specialized TreeMutation that specifically deals with pruning nodes. It takes an existing DecisionNode that is prunable and an updated_node to replace it.

GrowMutation Class: Another specialized TreeMutation, but this one is for growing the tree. It takes an existing LeafNode and an updated_node to replace it.
"""

class PruneMutation(TreeMutation):

    def __init__(self, existing_node, updated_node):
        if not type(existing_node) == DecisionNode or not existing_node.is_prunable():
            raise TypeError("Pruning only valid on prunable decision nodes")
        super().__init__("prune", existing_node, updated_node)

class GrowMutation(TreeMutation):

    def __init__(self, existing_node, updated_node):
        if type(existing_node) != LeafNode:
            raise TypeError("Can only grow Leaf nodes")
        super().__init__("grow", existing_node, updated_node)

"""mutate Function: This function applies a TreeMutation to a given Tree, altering its structure accordingly. The function first invalidates the tree's prediction cache. Depending on the kind of mutation ("prune" or "grow"), it then removes and/or adds nodes to the tree. Finally, it updates the parent-child relationships among the nodes."""

def mutate(tree, mutation) -> None:
    """
    Apply a change to the structure of the tree
    Modifies not only the tree, but also the links between the TreeNodes

    Parameters
    ----------
    tree: Tree
        The tree to mutate
    mutation: TreeMutation
        The mutation to apply to the tree
    """
    tree.cache_up_to_date = False

    if mutation.kind == "prune":
        tree.remove_node(mutation.existing_node)
        tree.remove_node(mutation.existing_node.left_child)
        tree.remove_node(mutation.existing_node.right_child)
        tree.add_node(mutation.updated_node)

    if mutation.kind == "grow":
        tree.remove_node(mutation.existing_node)
        tree.add_node(mutation.updated_node.left_child)
        tree.add_node(mutation.updated_node.right_child)
        tree.add_node(mutation.updated_node)

    for node in tree.nodes:
        if node.right_child == mutation.existing_node:
            node._right_child = mutation.updated_node
        if node.left_child == mutation.existing_node:
            node._left_child = mutation.updated_node

"""Initializer Class: This is an abstract class that serves as an interface for initializing the trees in the Bayesian Additive Regression Trees (BART) model. The default behavior is to leave the trees uninitialized.

SklearnTreeInitializer Class: This class inherits from Initializer and initializes the tree structure and leaf node values by fitting a single scikit-learn Gradient Boosting Regressor tree. It copies both the tree structure and leaf node parameters into the BART tree.


"""

class Initializer(object):
    """
    The abstract interface for the tree initializers.

    Initializers are responsible for setting the starting values of the model, in particular:
      - structure of decision and leaf nodes
      - variables and values used in splits
      - values of leaf nodes

    Good initialization of trees helps speed up convergence of sampling

    Default behaviour is to leave trees uninitialized
    """

    def initialize_tree(self, tree) -> None:
        pass

    def initialize_trees(self, trees) -> None:
        for tree in trees:
            self.initialize_tree(tree)

class SklearnTreeInitializer(Initializer):
    """
    Initialize tree structure and leaf node values by fitting a single Sklearn GBR tree

    Both tree structure and leaf node parameters are copied across
    """

    def __init__(self,
                 max_depth: int=4,
                 min_samples_split: int=2,
                 loss: str='ls'):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.loss = loss

    def initialize_tree(self,
                        tree) -> None:
        params = {
            'n_estimators': 1,
            'max_depth': self.max_depth,
            'min_samples_split': self.min_samples_split,
            'learning_rate': 0.8,
            'loss': self.loss
        }

        clf = GradientBoostingRegressor(**params)
        fit = clf.fit(tree.nodes[0].data.X.data, tree.nodes[0].data.y.data)
        sklearn_tree = fit.estimators_[0][0].tree_
        map_sklearn_tree_into_bartpy(tree, sklearn_tree)

"""map_sklearn_split_into_bartpy_split_conditions Function: This function takes an sklearn_tree and an index and maps the split conditions stored in scikit-learn's gradient boosted trees library to the BART representation. Since scikit-learn isn't available in this environment, this function is just a placeholder.

map_sklearn_tree_into_bartpy Function: This function takes a BART tree (bartpy_tree) and an scikit-learn tree (sklearn_tree) and maps the structure and values from the scikit-learn tree into the BART tree. The function uses a recursive search function to traverse the tree and apply the necessary mutations.
"""

def map_sklearn_split_into_bartpy_split_conditions(sklearn_tree, index: int):
    """
    Convert how a split is stored in sklearn's gradient boosted trees library to the bartpy representation

    Parameters
    ----------
    sklearn_tree: The full tree object
    index: The index of the node in the tree object

    Returns
    -------

    """
    return (
        SplitCondition(sklearn_tree.feature[index], sklearn_tree.threshold[index], le),
        SplitCondition(sklearn_tree.feature[index], sklearn_tree.threshold[index], gt)
    )

def map_sklearn_tree_into_bartpy(bartpy_tree, sklearn_tree):
    nodes = [None for x in sklearn_tree.children_left]
    nodes[0] = bartpy_tree.nodes[0]

    def search(index: int=0):

        left_child_index, right_child_index = sklearn_tree.children_left[index], sklearn_tree.children_right[index]

        if left_child_index == -1:  # Trees are binary splits, so only need to check left tree
            return

        searched_node: LeafNode = nodes[index]

        split_conditions = map_sklearn_split_into_bartpy_split_conditions(sklearn_tree, index)
        decision_node = split_node(searched_node, split_conditions)

        left_child: LeafNode = decision_node.left_child
        right_child: LeafNode = decision_node.right_child
        left_child.set_value(sklearn_tree.value[left_child_index][0][0])
        right_child.set_value(sklearn_tree.value[right_child_index][0][0])

        mutation = GrowMutation(searched_node, decision_node)
        mutate(bartpy_tree, mutation)

        nodes[index] = decision_node
        nodes[left_child_index] = decision_node.left_child
        nodes[right_child_index] = decision_node.right_child

        search(left_child_index)
        search(right_child_index)

    search()

"""TreeNode Class: This class represents a generic node in the decision tree. It contains information related to the data at that node and links to its child nodes.

LeafNode Class: Inherits from TreeNode. It represents a leaf node in the tree and is responsible for making predictions. It also has a method to check whether it can be split further.

DecisionNode Class: Inherits from TreeNode. It represents an internal node in the tree and mainly serves to tie the tree structure together.

split_node Function: Takes a LeafNode and a pair of SplitConditions to split it into a DecisionNode with two new LeafNode children.

deep_copy_node Function: Takes a TreeNode and returns a deep copy of it.
"""

class TreeNode(object):
    """
    A representation of a node in the Tree
    Contains two main types of information:
        - Data relevant for the node
        - Links to children nodes
    """
    def __init__(self, split, depth: int, left_child: 'TreeNode'=None, right_child: 'TreeNode'=None):
        self.depth = depth
        self._split = split
        self._left_child = left_child
        self._right_child = right_child

    @property
    def data(self):
        return self._split.data

    @property
    def left_child(self) -> 'TreeNode':
        return self._left_child

    @property
    def right_child(self) -> 'TreeNode':
        return self._right_child

    @property
    def split(self):
        return self._split

    def update_y(self, y):
        self.data.update_y(y)
        if self.left_child is not None:
            self.left_child.update_y(y)
            self.right_child.update_y(y)
alpha_prior, beta_prior = 1e29,1e29

class LeafNode(TreeNode):
    """
    A representation of a leaf node in the tree
    In addition to the normal work of a `Node`, a `LeafNode` is responsible for:
        - Interacting with `Data`
        - Making predictions
    """

    def __init__(self, split, depth=0, value=0.0):
        self._value = value
        super().__init__(split, depth, None, None)

    def set_value(self, value: float) -> None:
        self._value = value

    @property
    def current_value(self):
        return self._value

    def predict(self) -> float:
        return self.current_value

    def is_splittable(self) -> bool:
        return self.data.X.is_at_least_one_splittable_variable()

class DecisionNode(TreeNode):
    """
    A `DecisionNode` encapsulates internal node in the tree
    Unlike a `LeafNode`, it contains very little actual logic beyond tying the tree together
    """

    def __init__(self, split, left_child_node: TreeNode, right_child_node: TreeNode, depth=0):
        super().__init__(split, depth, left_child_node, right_child_node)

    def is_prunable(self) -> bool:
        return type(self.left_child) == LeafNode and type(self.right_child) == LeafNode

    def most_recent_split_condition(self):
        return self.left_child.split.most_recent_split_condition()

def split_node(node: LeafNode, split_conditions) -> DecisionNode:
    """
    Converts a `LeafNode` into an internal `DecisionNode` by applying the split condition
    The left node contains all values for the splitting variable less than the splitting value
    """
    left_split = node.split + split_conditions[0]
    split_conditions[1].carry_n_obsv = node.data.X.n_obsv - left_split.data.X.n_obsv
    split_conditions[1].carry_y_sum = node.data.y.summed_y() - left_split.data.y.summed_y()

    right_split = node.split + split_conditions[1]

    return DecisionNode(node.split,
                        LeafNode(left_split, depth=node.depth + 1),
                        LeafNode(right_split, depth=node.depth + 1),
                        depth=node.depth)

def deep_copy_node(node: TreeNode):
    if type(node) == LeafNode:
        node: LeafNode = node
        return LeafNode(node.split.out_of_sample_conditioner(), value=node.current_value, depth=node.depth)
    elif type(node) == DecisionNode:
        node: DecisionNode = node
        return DecisionNode(node.split.out_of_sample_conditioner(), node.left_child, node.right_child, depth=node.depth)
    else:
        raise TypeError("Unsupported node type")

"""## Sample Methods

Sampler Class (Abstract Base Class): This class defines an interface for samplers, which presumably handle the MCMC steps. The step method is abstract and meant to be implemented in subclasses.

Model Class: This is the core class that encapsulates the entire BART model. It maintains all the trees, the data, and other model-specific parameters like
�
σ,
�
α,
�
β, and
�
k. The class has methods for prediction, getting residuals, and refreshing trees. Importantly, it also has a method named refreshed_trees, which seems to be designed for updating the trees in the model.
"""

class Sampler(ABC):

    @abstractmethod
    def step(self, model, tree) -> bool:
        raise NotImplementedError()

class Model:

    def __init__(self,
                 data,
                 sigma,
                 trees=None,
                 n_trees: int = 50,
                 alpha: float = 0.95,
                 beta: float = 2.,
                 k: int = 2.,
                 initializer=SklearnTreeInitializer()):

        self.data = deepcopy(data)
        self.alpha = float(alpha)
        self.beta = float(beta)
        self.k = k
        self._sigma = sigma
        self._prediction = None
        self._initializer = initializer

        if trees is None:
            self.n_trees = n_trees
            self._trees = self.initialize_trees()
            if self._initializer is not None:
                self._initializer.initialize_trees(self.refreshed_trees())
        else:
            self.n_trees = len(trees)
            self._trees = trees

    def initialize_trees(self) -> List[Tree]:
        trees = [Tree([LeafNode(Split(deepcopy(self.data)))]) for _ in range(self.n_trees)]
        for tree in trees:
            tree.update_y(tree.update_y(self.data.y.values / self.n_trees))
        return trees

    def residuals(self) -> np.ndarray:
        return self.data.y.values - self.predict()

    def unnormalized_residuals(self) -> np.ndarray:
        return self.data.y.unnormalized_y - self.data.y.unnormalize_y(self.predict())

    def predict(self, X: np.ndarray = None) -> np.ndarray:
        if X is not None:
            return self._out_of_sample_predict(X)
        return np.sum([tree.predict() for tree in self.trees], axis=0)

    def _out_of_sample_predict(self, X: np.ndarray) -> np.ndarray:
        if type(X) == pd.DataFrame:
            X: pd.DataFrame = X
            X = X.values
        return np.sum([tree.predict(X) for tree in self.trees], axis=0)

    @property
    def trees(self):
        return self._trees

    def refreshed_trees(self):
        if self._prediction is None:
            self._prediction = self.predict()
        for tree in self._trees:
            self._prediction -= tree.predict()
            tree.update_y(self.data.y.values - self._prediction)
            yield tree
            self._prediction += tree.predict()

    @property
    def sigma_m(self) -> float:
        return 0.5 / (self.k * np.power(self.n_trees, 0.5))

    @property
    def sigma(self):
        return self._sigma

"""SplitCondition Class: This class represents a condition for splitting a node in a decision tree. It contains details like the variable being split (splitting_variable), the value on which to split (splitting_value), and an operator defining the condition. Additional information like carry_y_sum and carry_n_obsv are also stored, likely to facilitate faster calculations.

is_not_constant Function: This is a utility function that checks whether a given NumPy array (series) contains more than one unique value. This is probably used to check whether a node is splittable or not based on the feature values.
"""

class SplitCondition(object):
    """
    A representation of a split in feature space.
    The two main components are:

        - splitting_variable: which variable is being split on
        - splitting_value: the value being split on
                           all values less than or equal to this go left, all values greater go right

    """

    def __init__(self,
                 splitting_variable: int,
                 splitting_value: float,
                 operator,
                 condition=None,
                 carry_y_sum=None,
                 carry_n_obsv=None):
        self.splitting_variable = splitting_variable
        self.splitting_value = splitting_value
        self._condition = condition
        self.operator = operator

        self.carry_y_sum = carry_y_sum
        self.carry_n_obsv = carry_n_obsv

    def __str__(self):
        return str(self.splitting_variable) + ": " + str(self.splitting_value)

    def __eq__(self, other: 'SplitCondition'):
        return self.splitting_variable == other.splitting_variable and self.splitting_value == other.splitting_value and self.operator == other.operator

def is_not_constant(series: np.ndarray) -> bool:
    """
    Quickly identify whether a series contains more than 1 distinct value
    Parameters
    ----------
    series: np.ndarray
    The series to assess

    Returns
    -------
    bool
        True if more than one distinct value found
    """
    if len(series) <= 1:
        return False
    first_value = None
    for i in range(1, len(series)):
        # if not series.mask[i] and series.data[i] != first_value:
        if series[i] != first_value:
            if first_value is None:
                first_value = series.data[i]
            else:
                return True
    return False

"""he functions ensure_numpy_array, ensure_float_array, and format_covariate_matrix are utility functions aimed at ensuring that the input data are in the appropriate format (NumPy arrays and floats).

make_bartpy_data Function: This function takes in the feature matrix
�
X and target array
�
y, optionally normalizes them, and returns an instance of the Data class, which is presumably used to store the data in the BART model.
"""

def ensure_numpy_array(X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
    if isinstance(X, pd.DataFrame):
        return X.values
    else:
        return X

def ensure_float_array(X: np.ndarray) -> np.ndarray:
    return X.astype(float)

def format_covariate_matrix(X: Union[np.ndarray, pd.DataFrame]) -> np.ndarray:
    X = ensure_numpy_array(X)
    return ensure_float_array(X)

def make_bartpy_data(X: Union[np.ndarray, pd.DataFrame],
                     y: np.ndarray,
                     normalize: bool = True) -> 'Data':
    X = format_covariate_matrix(X)
    y = y.astype(float)
    return Data(X, y, normalize=normalize)

"""The CovariateMatrix class handles the covariate matrix
�
X and includes methods for various operations like:

get_column(i: int): Returns a specific column of the covariate matrix.
splittable_variables(): Returns a list of variables (features) that can be split upon. These are the variables that have more than one unique value.
is_at_least_one_splittable_variable(): Checks if there's at least one variable that can be split upon.
random_splittable_variable(): Randomly selects a splittable variable.
random_splittable_value(variable: int): Randomly selects a splittable value for a given variable.
update_mask(other: SplitCondition): Updates the mask based on a given SplitCondition.
"""

class CovariateMatrix(object):

    def __init__(self,
                 X: np.ndarray,
                 mask: np.ndarray,
                 n_obsv: int,
                 unique_columns: List[int],
                 splittable_variables: List[int]):

        if type(X) == pd.DataFrame:
            X: pd.DataFrame = X
            X = X.values

        self._X = X
        self._n_obsv = n_obsv
        self._n_features = X.shape[1]
        self._mask = mask

        # Cache iniialization
        if unique_columns is not None:
            self._unique_columns = [x if x is True else None for x in unique_columns]
        else:
            self._unique_columns = [None for _ in range(self._n_features)]
        if splittable_variables is not None:
            self._splittable_variables = [x if x is False else None for x in splittable_variables]
        else:
            self._splittable_variables = [None for _ in range(self._n_features)]
        self._max_values = [None] * self._n_features
        self._X_column_cache = [None] * self._n_features
        self._max_value_cache = [None] * self._n_features
        self._X_cache = None

    @property
    def mask(self) -> np.ndarray:
        return self._mask

    @property
    def values(self) -> np.ndarray:
        return self._X

    def get_column(self, i: int) -> np.ndarray:
        if self._X_cache is None:
            self._X_cache = self.values[~self.mask, :]
        return self._X_cache[:, i]

    def splittable_variables(self) -> List[int]:
        """
        List of columns that can be split on, i.e. that have more than one unique value

        Returns
        -------
        List[int]
            List of column numbers that can be split on
        """
        for i in range(0, self._n_features):
            if self._splittable_variables[i] is None:
                self._splittable_variables[i] = is_not_constant(self.get_column(i))

        return [i for (i, x) in enumerate(self._splittable_variables) if x is True]

    @property
    def n_splittable_variables(self) -> int:
        return len(self.splittable_variables())

    def is_at_least_one_splittable_variable(self) -> bool:
        if any(self._splittable_variables):
            return True
        else:
            return len(self.splittable_variables()) > 0

    def random_splittable_variable(self) -> str:
        """
        Choose a variable at random from the set of splittable variables
        Returns
        -------
            str - a variable name that can be split on
        """
        return np.random.choice(np.array(self.splittable_variables()), 1)[0]

    def is_column_unique(self, i: int) -> bool:
        """
        Identify whether feature contains only unique values, i.e. it has no duplicated values
        Useful to provide a faster way to calculate the probability of a value being selected in a variable

        Returns
        -------
        List[int]
        """
        if self._unique_columns[i] is None:
            self._unique_columns[i] = len(np.unique(self.get_column(i))) == self._n_obsv
        return self._unique_columns[i]

    def max_value_of_column(self, i: int):
        if self._max_value_cache[i] is None:
            self._max_value_cache[i] = self.get_column(i).max()
        return self._max_value_cache[i]

    def random_splittable_value(self, variable: int) -> Any:
        """
        Return a random value of a variable
        Useful for choosing a variable to split on

        Parameters
        ----------
        variable - str
            Name of the variable to split on

        Returns
        -------
        Any

        Notes
        -----
          - Won't create degenerate splits, all splits will have at least one row on both sides of the split
        """
        max_value = self.max_value_of_column(variable)
        candidate = np.random.choice(self.get_column(variable))
        while candidate == max_value:
            candidate = np.random.choice(self.get_column(variable))
        return candidate

    def proportion_of_value_in_variable(self, variable: int, value: float) -> float:
        if self.is_column_unique(variable):
            return 1. / self.n_obsv
        else:
            return float(np.mean(self.get_column(variable) == value))

    def update_mask(self, other: SplitCondition) -> np.ndarray:
        if other.operator == gt:
            column_mask = self.values[:, other.splitting_variable] <= other.splitting_value
        elif other.operator == le:
            column_mask = self.values[:, other.splitting_variable] > other.splitting_value
        else:
            raise TypeError("Operator type not matched, only {} and {} supported".format(gt, le))

        return self.mask | column_mask

    @property
    def variables(self) -> List[int]:
        return list(range(self._n_features))

    @property
    def n_obsv(self) -> int:
        return self._n_obsv

"""The Target class manages the target variable
�
y for the Bayesian Additive Regression Trees (BART) model. The key functionalities and attributes are:

normalize_y(y: np.ndarray): This static method normalizes
�
y into the range
[
−
0.5
,
0.5
]
[−0.5,0.5]. This normalization is useful for standardizing the prior distribution for the leaf parameters and
�
σ.
unnormalize_y(y: np.ndarray): This reverses the normalization applied by normalize_y.
summed_y(): This method returns the sum of
�
y values that are unmasked.
update_y(y): Updates the
�
y values in the object and invalidates the cache for summed
�
y.
"""

class Target(object):

    def __init__(self, y, mask, n_obsv, normalize, y_sum=None):

        if normalize:
            self.original_y_min, self.original_y_max = y.min(), y.max()
            self._y = self.normalize_y(y)
        else:
            self._y = y

        self._mask = mask
        self._inverse_mask_int = (~self._mask).astype(int)
        self._n_obsv = n_obsv

        if y_sum is None:
            self.y_sum_cache_up_to_date = False
            self._summed_y = None
        else:
            self.y_sum_cache_up_to_date = True
            self._summed_y = y_sum

    @staticmethod
    def normalize_y(y: np.ndarray) -> np.ndarray:
        """
        Normalize y into the range (-0.5, 0.5)
        Useful for allowing the leaf parameter prior to be 0, and to standardize the sigma prior

        Parameters
        ----------
        y - np.ndarray

        Returns
        -------
        np.ndarray

        Examples
        --------
        >>> Data.normalize_y([1, 2, 3])
        array([-0.5,  0. ,  0.5])
        """
        y_min, y_max = np.min(y), np.max(y)
        return -0.5 + ((y - y_min) / (y_max - y_min))

    def unnormalize_y(self, y: np.ndarray) -> np.ndarray:
        distance_from_min = y - (-0.5)
        total_distance = (self.original_y_max - self.original_y_min)
        return self.original_y_min + (distance_from_min * total_distance)

    @property
    def unnormalized_y(self) -> np.ndarray:
        return self.unnormalize_y(self.values)

    @property
    def normalizing_scale(self) -> float:
        return self.original_y_max - self.original_y_min

    def summed_y(self) -> float:
        if self.y_sum_cache_up_to_date:
            return self._summed_y
        else:
            self._summed_y = np.sum(self._y * self._inverse_mask_int)
            self.y_sum_cache_up_to_date = True
            return self._summed_y

    def update_y(self, y) -> None:
        self._y = y
        self.y_sum_cache_up_to_date = False

    @property
    def values(self):
        return self._y

"""The Data class encapsulates the data within a split of feature space in the Bayesian Additive Regression Trees (BART) model. It is mainly responsible for holding and managing the covariate matrix
�
X and the target variable
�
y. The functionalities and attributes include:

__init__: Initializes the Data object with
�
X,
�
y, and optional parameters for masking, normalization, and caching. It uses two other classes CovariateMatrix and Target for managing
�
X and
�
y, respectively.

y: Property that returns the Target object, which manages the target variable.

X: Property that returns the CovariateMatrix object, which manages the feature matrix.

mask: Property that returns the mask array indicating which observations are included in this split.

update_y(y: np.ndarray): Updates the target variable
�
y in the Target object.

__add__(other: SplitCondition): Adds a SplitCondition to the data, updating the mask and returning a new Data object representing the data after the split condition is applied.


"""

class Data:
    """
    Encapsulates the data within a split of feature space.
    Primarily used to cache computations on the data for better performance

    Parameters
    ----------
    X: np.ndarray
        The subset of the covariate matrix that falls into the split
    y: np.ndarray
        The subset of the target array that falls into the split
    normalize: bool
        Whether to map the target into -0.5, 0.5
    cache: bool
        Whether to cache common values.
        You really only want to turn this off if you're not going to the resulting object for anything (e.g. when testing)
    """

    def __init__(self,
                 X: np.ndarray,
                 y: np.ndarray,
                 mask: Optional[np.ndarray] = None,
                 normalize: bool = False,
                 unique_columns: List[int] = None,
                 splittable_variables: Optional[List[Optional[bool]]] = None,
                 y_sum: float = None,
                 n_obsv: int = None):

        if mask is None:
            mask = np.zeros_like(y).astype(bool)
        self._mask: np.ndarray = mask

        if n_obsv is None:
            n_obsv = (~self.mask).astype(int).sum()

        self._n_obsv = n_obsv

        self._X = CovariateMatrix(X, mask, n_obsv, unique_columns, splittable_variables)
        self._y = Target(y, mask, n_obsv, normalize, y_sum)

    @property
    def y(self) -> Target:
        return self._y

    @property
    def X(self) -> CovariateMatrix:
        return self._X

    @property
    def mask(self) -> np.ndarray:
        return self._mask

    def update_y(self, y: np.ndarray) -> None:
        self._y.update_y(y)

    def __add__(self, other: SplitCondition) -> 'Data':
        updated_mask = self.X.update_mask(other)

        return Data(self.X.values,
                    self.y.values,
                    updated_mask,
                    normalize=False,
                    unique_columns=self._X._unique_columns,
                    splittable_variables=self._X._splittable_variables,
                    y_sum=other.carry_y_sum,
                    n_obsv=other.carry_n_obsv)

"""## MCMC

The Sigma class in the Bayesian Additive Regression Trees (BART) model encapsulates the standard deviation term,

σ, in the model. This σ is related to the target variable

y as follows:


∼
Normal
(
sum of trees
,
�
)
y∼Normal(sum of trees,σ)
The class has the following functionalities and attributes:

__init__: Initializes the Sigma object with hyperparameters alpha and beta for the prior distribution of σ, which is assumed to follow an inverse gamma distribution. It also takes a scaling_factor, which is used to rescale the variance to the original scale if the data was normalized.

set_value(value: float): Sets the current value of σ.

current_value(): Returns the current value of
σ.

current_unnormalized_value(): Returns the current value of
�
σ after rescaling it back to the original scale using the scaling_factor.
"""

class Sigma:
    """
    A representation of the sigma term in the model.
    Specifically, this is the sigma of y itself, i.e. the sigma in
        y ~ Normal(sum_of_trees, sigma)

    The default prior is an inverse gamma distribution on the variance
    The parametrization is slightly different to the numpy gamma version, with the scale parameter inverted

    Parameters
    ----------
    alpha - the shape of the prior
    beta - the scale of the prior
    scaling_factor - the range of the original distribution
                     needed to rescale the variance into the original scale rather than on (-0.5, 0.5)

    """

    def __init__(self, alpha: float, beta: float, scaling_factor: float):
        self.alpha = alpha
        self.beta = beta
        self._current_value = 1.0
        self.scaling_factor = scaling_factor

    def set_value(self, value: float) -> None:
        self._current_value = value

    def current_value(self) -> float:
        return self._current_value

    def current_unnormalized_value(self) -> float:
        return self.current_value() * self.scaling_factor

"""UniformScalarSampler: A helper class for generating uniform random samples, optimized by caching a certain number of samples (cache_size) to avoid frequent calls to the random number generator."""

class UniformScalarSampler():

    def __init__(self,
                 cache_size: int=1000):
        self._cache_size = cache_size
        self._cache = []

    def sample(self):
        if len(self._cache) == 0:
            self.refresh_cache()
        return self._cache.pop()

    def refresh_cache(self):
        self._cache = list(np.random.uniform(size=self._cache_size))

"""The UnconstrainedTreeMutationSampler class serves as a Metropolis-Hastings sampler for mutating decision trees in the BART model. It takes proposals for mutations, evaluates their likelihood ratios, and decides whether to accept or reject the proposed mutations based on a random sample from a uniform distribution. The class is used to make MCMC (Markov Chain Monte Carlo) steps in the tree mutation space."""

class UnconstrainedTreeMutationSampler(Sampler):
    """
    A sampler for tree mutation space.
    Responsible for producing samples of ways to mutate a tree within a model

    Works by combining a proposer and likihood evaluator into:
     - propose a mutation
     - assess likihood
     - accept if likihood higher than a uniform(0, 1) draw

    Parameters
    ----------
    proposer: TreeMutationProposer
    likihood_ratio: TreeMutationLikihoodRatio
    """

    def __init__(self,
                 proposer,
                 likihood_ratio,
                 scalar_sampler=UniformScalarSampler()):
        self.proposer = proposer
        self.likihood_ratio = likihood_ratio
        self._scalar_sampler = scalar_sampler

    def sample(self, model: Model, tree):
        proposal = self.proposer.propose(tree)
        ratio = self.likihood_ratio.log_probability_ratio(model, tree, proposal)
        if self._scalar_sampler.sample() < ratio:
            return proposal
        else:
            return None

    def step(self, model: Model, tree):
        mutation = self.sample(model, tree)
        if mutation is not None:
            mutate(tree, mutation)
        return mutation

"""sample_split_condition(node: LeafNode) -> Tuple[SplitCondition, SplitCondition]:
This function performs the following operations:

Selects a variable to split upon randomly from the set of splittable variables. This is achieved using np.random.choice.
For the selected variable, a split value is sampled randomly using node.data.X.random_splittable_value.
If a valid split value is found, it returns two SplitCondition objects, one for 'less than or equal to' (le) and another for 'greater than' (gt). Otherwise, it returns None.
sample_split_node(node: LeafNode) -> DecisionNode:
This function uses the sample_split_condition function to sample a potential split condition and then splits the node into a decision node with two leaf children. It does so by using the split_node function.
"""

def sample_split_condition(node: LeafNode):
    """
    Randomly sample a splitting rule for a particular leaf node
    Works based on two random draws

      - draw a node to split on based on multinomial distribution
      - draw an observation within that variable to split on

    Returns None if there isn't a possible non-degenerate split
    """
    split_variable = np.random.choice(list(node.split.data.X.splittable_variables()))
    split_value = node.data.X.random_splittable_value(split_variable)
    if split_value is None:
        return None
    return SplitCondition(split_variable, split_value, le), SplitCondition(split_variable, split_value, gt)

def sample_split_node(node: LeafNode) -> DecisionNode:
    """
    Split a leaf node into a decision node with two leaf children
    The variable and value to split on is determined by sampling from their respective distributions
    """
    conditions = sample_split_condition(node)
    return split_node(node, conditions)

class NoSplittableVariableException(Exception):
    pass


class NoPrunableNodeException(Exception):
    pass

"""random_splittable_leaf_node(tree: Tree) -> LeafNode:

Finds all leaf nodes in the tree that can be split (i.e., have at least two distinct values in their covariate matrix).
Randomly chooses one such node and returns it.
Raises a NoSplittableVariableException if no such node exists.
random_prunable_decision_node(tree: Tree) -> DecisionNode:

Finds all decision nodes in the tree that can be pruned (i.e., have two leaf node children).
Randomly chooses one such node and returns it.
Raises a NoPrunableNodeException if no such node exists.
uniformly_sample_grow_mutation(tree: Tree) -> TreeMutation:

Uses random_splittable_leaf_node to find a splittable leaf node.
Uses sample_split_node to propose a new decision node that would replace the leaf node.
Returns a GrowMutation object that encapsulates the proposed change.
uniformly_sample_prune_mutation(tree: Tree) -> TreeMutation:

Uses random_prunable_decision_node to find a prunable decision node.
Proposes to convert the decision node back to a leaf node.
Returns a PruneMutation object that encapsulates the proposed change.

"""

def random_splittable_leaf_node(tree: Tree) -> LeafNode:
    """
    Returns a random leaf node that can be split in a non-degenerate way
    i.e. a random draw from the set of leaf nodes that have at least two distinct values in their covariate matrix
    """
    splittable_nodes = tree.splittable_leaf_nodes
    if len(splittable_nodes) == 0:
        raise NoSplittableVariableException()
    else:
        return np.random.choice(splittable_nodes)

def random_prunable_decision_node(tree: Tree) -> DecisionNode:
    """
    Returns a random decision node that can be pruned
    i.e. a random draw from the set of decision nodes that have two leaf node children
    """
    leaf_parents = tree.prunable_decision_nodes
    if len(leaf_parents) == 0:
        raise NoPrunableNodeException()
    return np.random.choice(leaf_parents)

def uniformly_sample_grow_mutation(tree: Tree) -> TreeMutation:
    node = random_splittable_leaf_node(tree)
    updated_node = sample_split_node(node)
    return GrowMutation(node, updated_node)

def uniformly_sample_prune_mutation(tree: Tree) -> TreeMutation:
    node = random_prunable_decision_node(tree)
    updated_node = LeafNode(node.split, depth=node.depth)
    return PruneMutation(node, updated_node)

"""
The TreeMutationProposer class is an abstract base class (ABC) that serves as a template for classes that propose tree mutations. It has an abstract method called propose, which takes a tree as input and returns a TreeMutation object that represents a proposed modification to the tree. Concrete implementations of this class must provide the actual logic for proposing tree mutations by overriding this method.

The DiscreteSampler class provides functionality for sampling from a discrete set of values. The values and their associated probabilities are passed as arguments during initialization:

values: A list of possible values to sample from.
probas: A list of probabilities corresponding to the values in values. If this is not provided, uniform probabilities are assumed.
The class also uses caching to speed up the sampling process. The number of values to cache is specified by the cache_size parameter. The sample method picks one value from the cache, and the cache is refreshed when it becomes empty."""

class TreeMutationProposer(ABC):
    """
    A TreeMutationProposer is responsible for generating samples from tree space
    It is capable of generating proposed TreeMutations
    """

    @abstractmethod
    def propose(self, tree: Tree) -> TreeMutation:
        """
        Propose a mutation to make to the given tree

        Parameters
        ----------
        tree: Tree
            The tree to be mutate

        Returns
        -------
        TreeMutation
            A way to update the input tree
        """
        raise NotImplementedError()

class DiscreteSampler():

    def __init__(self,
                 values: List[Any],
                 probas: List[float]=None,
                 cache_size: int=1000):
        self._values = values
        if probas is None:
            probas = [1.0 / len(values) for x in values]
        self._probas = probas
        self._cache_size = cache_size
        self._cache = []

    def sample(self):
        if len(self._cache) == 0:
            self.refresh_cache()
        return self._cache.pop()

    def refresh_cache(self):
        self._cache = list(np.random.choice(self._values, p=self._probas, size=self._cache_size))

"""
The UniformMutationProposer class is a concrete implementation of the abstract base class TreeMutationProposer. This class is responsible for proposing mutations to a given tree (Tree). The mutations can be either growing the tree by adding a new node (uniformly_sample_grow_mutation) or pruning the tree by removing a node (uniformly_sample_prune_mutation)."""

class UniformMutationProposer(TreeMutationProposer):

    def __init__(self,
                 prob_method: List[float]=None,
                 prob_method_lookup=None):
        if prob_method_lookup is not None:
            self.prob_method_lookup = prob_method_lookup
        else:
            if prob_method is None:
                prob_method = [0.5, 0.5]
            self.prob_method_lookup = {x[0]: x[1] for x in zip([uniformly_sample_grow_mutation, uniformly_sample_prune_mutation], prob_method)}
        self.methods = list(self.prob_method_lookup.keys())
        self.method_sampler = DiscreteSampler(list(self.prob_method_lookup.keys()),
                                              list(self.prob_method_lookup.values()),
                                              cache_size=1000)

    def propose(self, tree: Tree) -> TreeMutation:
        method = self.method_sampler.sample()
        try:
            return method(tree)
        except NoSplittableVariableException:
            return self.propose(tree)
        except NoPrunableNodeException:
            return self.propose(tree)

"""The TreeMutationLikihoodRatio class is an abstract base class (ABC) designed to evaluate the likelihood ratios for different tree mutations in a Bayesian Additive Regression Trees (BART) model or a similar tree-based model. The class outlines the methods that must be implemented by any concrete subclass. These methods are intended to return the log-likelihood ratios for various aspects of tree mutations."""

class TreeMutationLikihoodRatio(ABC):

    def log_probability_ratio(self, model: Model, tree: Tree, mutation: TreeMutation) -> float:
        return self.log_transition_ratio(tree, mutation) + self.log_likihood_ratio(model, tree, mutation) + self.log_tree_ratio(model, tree, mutation)

    @abstractmethod
    def log_transition_ratio(self, tree: Tree, mutation: TreeMutation) -> float:
        raise NotImplementedError()

    @abstractmethod
    def log_tree_ratio(self, model: Model, tree: Tree, mutation: TreeMutation) -> float:
        raise NotImplementedError()

    @abstractmethod
    def log_likihood_ratio(self, model: Model, tree: Tree, mutation: TreeMutation):
        raise NotImplementedError()

class UniformTreeMutationLikihoodRatio(TreeMutationLikihoodRatio):

    def __init__(self,
                 prob_method: List[float]=None):
        if prob_method is None:
            prob_method = [0.5, 0.5]
        self.prob_method = prob_method

    def log_transition_ratio(self, tree: Tree, mutation: TreeMutation):
        if mutation.kind == "prune":
            mutation: PruneMutation = mutation
            return self.log_prune_transition_ratio(tree, mutation)
        if mutation.kind == "grow":
            mutation: GrowMutation = mutation
            return self.log_grow_transition_ratio(tree, mutation)
        else:
            raise NotImplementedError("kind {} not supported".format(mutation.kind))

    def log_tree_ratio(self, model: Model, tree: Tree, mutation: TreeMutation):
        if mutation.kind == "grow":
            mutation: GrowMutation = mutation
            return self.log_tree_ratio_grow(model, tree, mutation)
        if mutation.kind == "prune":
            mutation: PruneMutation = mutation
            return self.log_tree_ratio_prune(model, mutation)

    def log_likihood_ratio(self, model: Model, tree: Tree, proposal: TreeMutation):
        if proposal.kind == "grow":
            proposal: GrowMutation = proposal
            return self.log_likihood_ratio_grow(model, proposal)
        if proposal.kind == "prune":
            proposal: PruneMutation = proposal
            return self.log_likihood_ratio_prune(model, proposal)
        else:
            raise NotImplementedError("Only prune and grow mutations supported")

    @staticmethod
    def log_likihood_ratio_grow(model: Model, proposal: TreeMutation):
        return log_grow_ratio(proposal.existing_node, proposal.updated_node.left_child, proposal.updated_node.right_child, model.sigma, model.sigma_m)

    @staticmethod
    def log_likihood_ratio_prune(model: Model, proposal: TreeMutation):
        return - log_grow_ratio(proposal.updated_node, proposal.existing_node.left_child, proposal.existing_node.right_child, model.sigma, model.sigma_m)

    def log_grow_transition_ratio(self, tree: Tree, mutation: GrowMutation):
        prob_prune_selected = - np.log(n_prunable_decision_nodes(tree) + 1)
        prob_grow_selected = log_probability_split_within_tree(tree, mutation)

        prob_selection_ratio = prob_prune_selected - prob_grow_selected
        prune_grow_ratio = np.log(self.prob_method[1] / self.prob_method[0])

        return prune_grow_ratio + prob_selection_ratio

    def log_prune_transition_ratio(self, tree: Tree, mutation: PruneMutation):
        if n_splittable_leaf_nodes(tree) == 1:
            prob_grow_node_selected = - np.inf  # Infinitely unlikely to be able to grow a null tree
        else:
            prob_grow_node_selected = - np.log(n_splittable_leaf_nodes(tree) - 1)
        prob_split = log_probability_split_within_node(GrowMutation(mutation.updated_node, mutation.existing_node))
        prob_grow_selected = prob_grow_node_selected + prob_split

        prob_prune_selected = - np.log(n_prunable_decision_nodes(tree))

        prob_selection_ratio = prob_grow_selected - prob_prune_selected
        grow_prune_ratio = np.log(self.prob_method[0] / self.prob_method[1])

        return grow_prune_ratio + prob_selection_ratio

    @staticmethod
    def log_tree_ratio_grow(model: Model, tree: Tree, proposal: GrowMutation):
        denominator = log_probability_node_not_split(model, proposal.existing_node)

        prob_left_not_split = log_probability_node_not_split(model, proposal.updated_node.left_child)
        prob_right_not_split = log_probability_node_not_split(model, proposal.updated_node.right_child)
        prob_updated_node_split = log_probability_node_split(model, proposal.updated_node)
        prob_chosen_split = log_probability_split_within_tree(tree, proposal)
        numerator = prob_left_not_split + prob_right_not_split + prob_updated_node_split + prob_chosen_split

        return numerator - denominator

    @staticmethod
    def log_tree_ratio_prune(model: Model, proposal: PruneMutation):
        numerator = log_probability_node_not_split(model, proposal.updated_node)

        prob_left_not_split = log_probability_node_not_split(model, proposal.existing_node.left_child)
        prob_right_not_split = log_probability_node_not_split(model, proposal.existing_node.left_child)
        prob_updated_node_split = log_probability_node_split(model, proposal.existing_node)
        prob_chosen_split = log_probability_split_within_node(GrowMutation(proposal.updated_node, proposal.existing_node))
        denominator = prob_left_not_split + prob_right_not_split + prob_updated_node_split + prob_chosen_split

        return numerator - denominator

"""log_grow_ratio: This function calculates the log of the likelihood ratio between the proposed "grow" mutation and the current state of the node. The formula seems to be derived from the normal distribution's likelihood under the Bayesian framework, incorporating both the data and the priors (through sigma, the current value of the standard deviation of the noise, and sigma_mu, the standard deviation of the prior on the leaf parameters).

n_prunable_decision_nodes and n_splittable_leaf_nodes: These functions count the number of decision nodes that can be pruned and the number of leaf nodes that can be split, respectively. They are essential for determining the available moves in the MCMC space and calculating transition probabilities.

log_probability_split_within_tree and log_probability_split_within_node: These functions calculate the log-probabilities of selecting a particular node to split (conditional on the tree) and a specific splitting variable and value (conditional on the node), respectively. These probabilities are part of the Metropolis-Hastings acceptance ratio.
"""

def log_grow_ratio(combined_node: LeafNode, left_node: LeafNode, right_node: LeafNode, sigma: Sigma, sigma_mu: float):
    var = np.power(sigma.current_value(), 2)
    var_mu = np.power(sigma_mu, 2)
    n = combined_node.data.X.n_obsv
    n_l = left_node.data.X.n_obsv
    n_r = right_node.data.X.n_obsv

    first_term = (var * (var + n * sigma_mu)) / ((var + n_l * var_mu) * (var + n_r * var_mu))
    first_term = np.log(np.sqrt(first_term))

    combined_y_sum = combined_node.data.y.summed_y()
    left_y_sum = left_node.data.y.summed_y()
    right_y_sum = right_node.data.y.summed_y()

    left_resp_contribution = np.square(left_y_sum) / (var + n_l * sigma_mu)
    right_resp_contribution = np.square(right_y_sum) / (var + n_r * sigma_mu)
    combined_resp_contribution = np.square(combined_y_sum) / (var + n * sigma_mu)

    resp_contribution = left_resp_contribution + right_resp_contribution - combined_resp_contribution

    return first_term + ((var_mu / (2 * var)) * resp_contribution)

def n_prunable_decision_nodes(tree: Tree) -> int:
    return len(tree.prunable_decision_nodes)
def n_splittable_leaf_nodes(tree: Tree) -> int:
    return len(tree.splittable_leaf_nodes)
def log_probability_split_within_tree(tree: Tree, mutation: GrowMutation) -> float:
    prob_node_chosen_to_split_on = - np.log(n_splittable_leaf_nodes(tree))
    prob_split_chosen = log_probability_split_within_node(mutation)
    return prob_node_chosen_to_split_on + prob_split_chosen
def log_probability_split_within_node(mutation: GrowMutation) -> float:
    """
    The log probability of the particular grow mutation being selected conditional on growing a given node

    i.e.
    log(P(splitting_value | splitting_variable, node, grow) * P(splitting_variable | node, grow))
    """
    prob_splitting_variable_selected = - np.log(mutation.existing_node.data.X.n_splittable_variables)
    splitting_variable = mutation.updated_node.most_recent_split_condition().splitting_variable
    splitting_value = mutation.updated_node.most_recent_split_condition().splitting_value
    prob_value_selected_within_variable = np.log(mutation.existing_node.data.X.proportion_of_value_in_variable(splitting_variable, splitting_value))
    return prob_splitting_variable_selected + prob_value_selected_within_variable

"""log_probability_node_split and log_probability_node_not_split: These functions calculate the log-probabilities of a given tree node being split or not split, respectively. The model's hyperparameters alpha and beta are used in these calculations. The probability of a node being split decreases with its depth, which is a common regularization method in Bayesian decision trees to avoid overfitting.

get_tree_sampler: This function returns an instance of UnconstrainedTreeMutationSampler initialized with a UniformMutationProposer and a UniformTreeMutationLikihoodRatio. The probabilities p_grow and p_prune are used both for proposing mutations (grow or prune a node) and for calculating the likelihood ratios. This function encapsulates the creation of the sampler object, which is central to the MCMC algorithm.
"""

def log_probability_node_split(model: Model, node: TreeNode):
    return np.log(model.alpha * np.power(1 + node.depth, -model.beta))
def log_probability_node_not_split(model: Model, node: TreeNode):
    return np.log(1. - model.alpha * np.power(1 + node.depth, -model.beta))
def get_tree_sampler(p_grow: float,
                     p_prune: float) -> Sampler:
    proposer = UniformMutationProposer([p_grow, p_prune])
    likihood = UniformTreeMutationLikihoodRatio([p_grow, p_prune])
    return UnconstrainedTreeMutationSampler(proposer, likihood)



"""The TreeMutation class and the deep copy functions (deep_copy_node, deep_copy_model, deep_copy_tree) provide additional utilities that are important for the operation of an MCMC sampler in the context of decision trees.

TreeMutation: This class serves as a container for a proposed change to the tree structure. It stores the existing node, the updated node, and the kind of change ("grow" or "prune"). This encapsulation is essential for the MCMC algorithm to propose updates to the model and to potentially reverse them if they are not accepted.

deep_copy_node: This function creates a deep copy of a tree node (TreeNode, which can be either a LeafNode or DecisionNode). This is important for creating a new tree structure that can be manipulated without affecting the original tree, which is essential for the MCMC algorithm.

deep_copy_model: This function creates a deep copy of the model, which includes deep copies of all the trees in the model. Again, this is essential for the MCMC algorithm, which needs to propose updates to the model that can be accepted or rejected.

deep_copy_tree: This function is used to create a deep copy of a tree. It utilizes deep_copy_node to create deep copies of all the nodes in the tree. This allows the MCMC algorithm to propose updates to individual trees without affecting the original model.
"""

class TreeMutation(object):
    def __init__(self, kind: str, existing_node, updated_node):
        self.kind = kind
        self.existing_node = existing_node
        self.updated_node = updated_node
    def __str__(self):
        return "{} - {} => {}".format(self.kind, self.existing_node, self.updated_node)

def deep_copy_node(node: TreeNode):
    if type(node) == LeafNode:
        node: LeafNode = node
        return LeafNode(node.split.out_of_sample_conditioner(), value=node.current_value, depth=node.depth)
    elif type(node) == DecisionNode:
        node: DecisionNode = node
        return DecisionNode(node.split.out_of_sample_conditioner(), node.left_child, node.right_child, depth=node.depth)
    else:
        raise TypeError("Unsupported node type")
def deep_copy_model(model: Model) -> Model:
    copied_model = Model(None, deepcopy(model.sigma), [deep_copy_tree(tree) for tree in model.trees])
    return copied_model
def deep_copy_tree(tree):
    return Tree([deep_copy_node(x) for x in tree.nodes])

class TraceLogger():

    def __init__(self,
                 f_tree_mutation_log=lambda x: x is not None,
                 f_model_log=lambda x: deep_copy_model(x),
                 f_in_sample_prediction_log=lambda x: x):
        self.f_tree_mutation_log = f_tree_mutation_log
        self.f_model_log = f_model_log
        self.f_in_sample_prediction_log = f_in_sample_prediction_log

    def __getitem__(self, item: str):
        if item == "Tree":
            return self.f_tree_mutation_log
        if item == "Model":
            return self.f_model_log
        if item == "In Sample Prediction":
            return self.f_in_sample_prediction_log
        if item in ["Node", "Sigma"]:
            return lambda x: None
        else:
            raise KeyError("No method for key {}".format(item))

"""The TraceLogger class is a utility designed for logging various aspects of the MCMC algorithm. Logging is critical in MCMC simulations for diagnosing the performance of the sampler, monitoring convergence, and storing the posterior samples for later analysis.

Here's a breakdown of its components:

f_tree_mutation_log: A lambda function that logs information about tree mutations. It's initialized to log only when the mutation (x) is not None.

f_model_log: A lambda function that logs a deep copy of the model, which would be useful for tracking the state of the model at each step of the MCMC algorithm.

f_in_sample_prediction_log: A lambda function that logs in-sample predictions. This could be useful for diagnosing the fit of the model.

__getitem__: Allows the user to fetch the logging function for a particular item (like "Tree", "Model", "In Sample Prediction", etc.) using square bracket notation.

## Model Sampler
"""

class ModelSampler(Sampler):

    def __init__(self,
                 schedule,
                 trace_logger_class=TraceLogger):
        self.schedule = schedule
        self.trace_logger_class = trace_logger_class

    def step(self, model: Model, trace_logger):
        step_result = defaultdict(list)
        for step_kind, step in self.schedule.steps(model):
            result = step()
            log_message = trace_logger[step_kind](result)
            if log_message is not None:
                step_result[step_kind].append(log_message)
        return {x: np.mean([1 if y else 0 for y in step_result[x]]) for x in step_result}

    def samples(self, model: Model,
                n_samples: int,
                n_burn: int,
                thin: float = 0.1,
                store_in_sample_predictions: bool = True,
                store_acceptance: bool = True):
        print("Starting burn")

        trace_logger = self.trace_logger_class()

        for _ in tqdm(range(n_burn)):
            self.step(model, trace_logger)
        trace = []
        model_trace = []
        acceptance_trace = []
        print("Starting sampling")

        thin_inverse = 1. / thin

        for ss in tqdm(range(n_samples)):
            step_trace_dict = self.step(model, trace_logger)
            if ss % thin_inverse == 0:
                if store_in_sample_predictions:
                    in_sample_log = trace_logger["In Sample Prediction"](model.predict())
                    if in_sample_log is not None:
                        trace.append(in_sample_log)
                if store_acceptance:
                    acceptance_trace.append(step_trace_dict)
                model_log = trace_logger["Model"](model)
                if model_log is not None:
                    model_trace.append(model_log)
        return {
            "model": model_trace,
            "acceptance": acceptance_trace,
            "in_sample_predictions": trace
        }

"""The SampleSchedule class manages the sequence of steps to be executed in each iteration of a Gibbs sampling algorithm. Gibbs sampling is a Markov chain Monte Carlo (MCMC) technique used for approximating the joint posterior distribution of a set of variables. The algorithm proceeds by iteratively sampling from the conditional distribution of each variable given all other variables. The SampleSchedule class schedules these conditional sampling steps."""



"""## SampleSchedule"""

class SampleSchedule:
    def __init__(self,
                 tree_sampler,
                 leaf_sampler,
                 sigma_sampler,
                 tree_sampler_hmc=None,
                 leaf_sampler_hmc=None,
                 sigma_sampler_hmc=None,
                 tree_sampler_nuts=None,
                 leaf_sampler_nuts=None,
                 sigma_sampler_nuts=None,
                 tree_sampler_adap=None,
                 leaf_sampler_adap=None,
                 sigma_sampler_adap=None,
                 burn_in=100):
        self.leaf_sampler = leaf_sampler
        self.sigma_sampler = sigma_sampler
        self.tree_sampler = tree_sampler
        self.leaf_sampler_hmc = leaf_sampler_hmc or leaf_sampler
        self.sigma_sampler_hmc = sigma_sampler_hmc or sigma_sampler
        self.tree_sampler_hmc = tree_sampler_hmc or tree_sampler
        self.leaf_sampler_nuts = leaf_sampler_nuts or leaf_sampler
        self.sigma_sampler_nuts = sigma_sampler_nuts or sigma_sampler
        self.tree_sampler_nuts = tree_sampler_nuts or tree_sampler
        self.tree_sampler_adap = tree_sampler_adap or tree_sampler
        self.leaf_sampler_adap = leaf_sampler_adap or leaf_sampler
        self.sigma_sampler_adap = sigma_sampler_adap or sigma_sampler
        self.burn_in = burn_in
        self.current_iter = 0
        self.method = "default"

    def set_method(self, method):
        self.method = method

    def steps(self, model: Model):
        self.current_iter += 1

        sampler_dict = {
            "default": (self.tree_sampler, self.leaf_sampler, self.sigma_sampler),
            "Adaptive": (self.tree_sampler_adap, self.leaf_sampler_adap, self.sigma_sampler_adap),
            "HMC": (self.tree_sampler_hmc, self.leaf_sampler_hmc, self.sigma_sampler_hmc),
            "NUTS": (self.tree_sampler_nuts, self.leaf_sampler_nuts, self.sigma_sampler_nuts)
        }

        tree_sampler, leaf_sampler, sigma_sampler = sampler_dict[self.method]

        for tree in model.refreshed_trees():
            yield "Tree", lambda: print(f"Debug Tree {self.method}: {type(tree_sampler.step(model, tree))}") or tree_sampler.step(model, tree)

            for leaf_node in tree.leaf_nodes:
                yield "Node", lambda: print(f"Debug Node {self.method}: {type(leaf_sampler.step(model, leaf_node))}") or leaf_sampler.step(model, leaf_node)

        yield "Node", lambda: print(f"Debug Sigma {self.method}: {type(sigma_sampler.step(model, model.sigma))}") or sigma_sampler.step(model, model.sigma)

"""## Traditional Backfitting"""

class SigmaSampler(Sampler):

    def step(self, model: Model, sigma: Sigma) -> float:
        sample_value = self.sample(model, sigma)
        sigma.set_value(sample_value)
        return sample_value

    @staticmethod
    def sample(model: Model, sigma: Sigma) -> float:
        posterior_alpha = sigma.alpha + (model.data.X.n_obsv / 2.)
        posterior_beta = sigma.beta + (0.5 * (np.sum(np.square(model.residuals()))))
        draw = np.power(np.random.gamma(posterior_alpha, 1. / posterior_beta), -0.5)
        return draw

class NormalScalarSampler():

    def __init__(self,
                 cache_size: int = 1000):
        self._cache_size = cache_size
        self._cache = []

    def sample(self):
        if len(self._cache) == 0:
            self.refresh_cache()
        return self._cache.pop()

    def refresh_cache(self):
        self._cache = list(np.random.normal(size=self._cache_size))

class LeafNodeSampler(Sampler):
    def __init__(self,
                 scalar_sampler=NormalScalarSampler(60000)):
        self._scalar_sampler = scalar_sampler

    def step(self, model: Model, node) -> float:
        sampled_value = self.sample(model, node)
        node.set_value(sampled_value)
        return sampled_value

    def sample(self, model: Model, node) -> float:
        prior_var = model.sigma_m ** 2
        n = node.data.X.n_obsv
        likihood_var = (model.sigma.current_value() ** 2) / n
        likihood_mean = node.data.y.summed_y() / n
        posterior_variance = 1. / (1. / prior_var + 1. / likihood_var)
        posterior_mean = likihood_mean * (prior_var / (likihood_var + prior_var))
        return posterior_mean + (self._scalar_sampler.sample() * np.power(posterior_variance / model.n_trees, 0.5))

## Adaptive MCMC

"""## Adaptive MCMC"""

import scipy.stats

class AdaptiveSigmaSampler(SigmaSampler):
    def __init__(self, init_var_sigma = 0.05, adapt_window=5):
        self.variance = init_var_sigma  # Initialize the proposal distribution variance
        self.adapt_window = adapt_window
        self.accepted_samples = []

    def step(self, model: Model, sigma: Sigma) -> float:
        sampled_value = self.sample(model, sigma)
        sigma.set_value(sampled_value)
        return sampled_value

    def sample(self, model: Model, sigma: Sigma) -> float:
        current_sigma = sigma.current_value()

        # Generate proposal based on the current sigma and adaptive variance
        proposal_sigma = current_sigma + np.random.normal(0, np.sqrt(self.variance))

        # Compute log-posterior for the current_sigma
        posterior_alpha_current = sigma.alpha + (model.data.X.n_obsv / 2.)
        posterior_beta_current = sigma.beta + (0.5 * np.sum(np.square(model.residuals())))
        log_prior_current = scipy.stats.gamma.logpdf(current_sigma, a=posterior_alpha_current, scale=1.0 / posterior_beta_current)
        log_likelihood_current = -0.5 * model.data.X.n_obsv * np.log(current_sigma ** 2) - (1.0 / (2 * current_sigma ** 2)) * np.sum(np.square(model.residuals()))
        log_posterior_current = log_prior_current + log_likelihood_current

        # Compute log-posterior for the proposal_sigma
        log_prior_proposal = scipy.stats.gamma.logpdf(proposal_sigma, a=posterior_alpha_current, scale=1.0 / posterior_beta_current)  # Note that posterior_alpha and posterior_beta would be the same for proposal_sigma

        log_likelihood_proposal = -0.5 * model.data.X.n_obsv * np.log(proposal_sigma ** 2) - (1.0 / (2 * proposal_sigma ** 2)) * np.sum(np.square(model.residuals()))
        log_posterior_proposal = log_prior_proposal + log_likelihood_proposal
        mh_ratio = np.exp(log_posterior_proposal - log_posterior_current)
        print(f"Debug: mh_ratio: {mh_ratio}")
        if np.random.rand() < mh_ratio:
            new_sigma = proposal_sigma
            self.accepted_samples.append(new_sigma)
        else:
            new_sigma = current_sigma
        # Adapt the proposal distribution variance
        if len(self.accepted_samples) % self.adapt_window == 0:
            self.variance = np.var(self.accepted_samples)
        return new_sigma

class AdaptiveLeafNodeSampler(Sampler):
    def __init__(self, init_var_leaf=0.05, adapt_window=50):
        self.init_var_leaf = init_var_leaf
        self.adapt_window = adapt_window
        self.accepted_samples = []

    def step(self, model: Model, node) -> float:
        sampled_value = self.sample(model, node)
        node.set_value(sampled_value)
        return sampled_value

    def sample(self, model: Model, node) -> float:
        current_theta = node.current_value
        #print(f"Debug: current_theta at the start: {current_theta}")
        prior_var = model.sigma_m ** 2
        n = node.data.X.n_obsv
        likihood_var = (model.sigma.current_value() ** 2) / n
        likihood_mean = node.data.y.summed_y() / n
        #print(f"Debug: self.init_var_leaf: {self.init_var_leaf}")
        #print(f"Debug: self.accepted_samples: {self.accepted_samples}")
        # Generate proposal
        proposal_theta = current_theta + np.random.normal(0, np.sqrt(self.init_var_leaf))
        #print(f"Debug: proposal_theta: {proposal_theta}")

        # Compute log posterior for current_theta
        log_posterior_current = self.log_posterior(current_theta, likihood_mean, likihood_var, prior_var, n)

        # Compute log posterior for proposal_theta
        log_posterior_proposal = self.log_posterior(proposal_theta, likihood_mean, likihood_var, prior_var, n)

        # Compute Metropolis-Hastings ratio
        mh_ratio = np.exp(log_posterior_proposal - log_posterior_current)
        # Accept or reject
        if np.random.rand() < mh_ratio:
            new_theta = proposal_theta
            self.accepted_samples.append(new_theta)
        else:
            new_theta = current_theta
        if len(self.accepted_samples) % self.adapt_window == 0:
            if len(self.accepted_samples) > 0:
                self.init_var_leaf = np.var(self.accepted_samples)
            else:
                print("Warning: self.accepted_samples is empty. Not updating self.init_var_leaf.")

        return new_theta

    def log_posterior(self, theta_value, likihood_mean, likihood_var, prior_var, n):

        log_prior = -0.5 * np.log(2 * np.pi * prior_var) - (theta_value ** 2) / (2 * prior_var)

        # Log likelihood
        log_likelihood = - (n / 2) * np.log(2 * np.pi * likihood_var) - \
                        (1 / (2 * likihood_var)) * n * (likihood_mean - theta_value) ** 2

        # Log likelihood
        log_likelihood = - (n / 2) * np.log(2 * np.pi * likihood_var) - \
                         (1 / (2 * likihood_var)) * n * (likihood_mean - theta_value) ** 2
        assert not np.isnan(log_prior + log_likelihood)
        return log_prior + log_likelihood

"""## HMC

### Sigma
"""

class SigmaSamplerHMC(Sampler):

    def __init__(self, leapfrog_steps, step_size):
        self.leapfrog_steps = leapfrog_steps
        self.step_size = step_size
        self.accepted_count = 0
        self.total_count = 0

    def step(self, model, sigma):
        # Update alpha and beta based on the data and the prior (like in the Gibbs sampler)
        posterior_alpha = sigma.alpha + (model.data.X.n_obsv / 2.)
        posterior_beta = sigma.beta + (0.5 * (np.sum(np.square(model.residuals()))))

        # Run HMC sampling
        sample_value = self.hmc_sample(model, sigma, posterior_alpha, posterior_beta)
        sigma.set_value(sample_value)
        return sample_value

    def log_posterior(self, sigma_value, alpha, beta, model):
        log_prior = alpha * np.log(beta) - gammaln(alpha) - (alpha + 1) * np.log(sigma_value) - beta / sigma_value
        n = model.data.X.n_obsv
        residuals = model.residuals()
        log_likelihood = - (n / 2) * np.log(2 * np.pi * sigma_value ** 2) - (1 / (2 * sigma_value ** 2)) * np.sum(np.square(residuals))
        return log_prior + log_likelihood

    def log_posterior_gradient(self, sigma_value, alpha, beta, model):
        log_prior_grad = -(alpha + 1) / sigma_value + beta / (sigma_value ** 2)
        n = model.data.X.n_obsv
        residuals = model.residuals()
        log_likelihood_grad = -n / sigma_value + (1 / (sigma_value ** 3)) * np.sum(np.square(residuals))
        return log_prior_grad + log_likelihood_grad

    def hamiltonian_dynamics(self, current_sigma, current_momentum, alpha, beta, model):
        new_sigma = current_sigma
        new_momentum = current_momentum
        for _ in range(self.leapfrog_steps):
            new_momentum -= 0.5 * self.step_size * self.log_posterior_gradient(new_sigma, alpha, beta, model)
            new_sigma += self.step_size * new_momentum
            new_momentum -= 0.5 * self.step_size * self.log_posterior_gradient(new_sigma, alpha, beta, model)
        return new_sigma, -new_momentum

    def hmc_sample(self, model, sigma, alpha, beta):
        current_sigma = sigma.current_value()
        current_momentum = np.random.normal(0, 1)

        new_sigma, new_momentum = self.hamiltonian_dynamics(current_sigma, current_momentum, alpha, beta, model)

        old_hamiltonian = -self.log_posterior(current_sigma, alpha, beta, model)
        new_hamiltonian = -self.log_posterior(new_sigma, alpha, beta, model)

        self.total_count += 1

        u = np.random.uniform(0, 1)
        mh_ratio = np.exp(old_hamiltonian - new_hamiltonian)
        print('mh_ratio: ', mh_ratio)

        if mh_ratio > u:
            self.accepted_count += 1
            return new_sigma
        else:
            self.step_size *= 0.95
            return current_sigma

"""### Leaf Node"""

class LeafNodeSamplerHMC(Sampler):

    def __init__(self, leapfrog_steps, step_size):
        self.leapfrog_steps = leapfrog_steps
        self.step_size = step_size

    def step(self, model: Model, node) -> float:
        prior_var = model.sigma_m ** 2
        n = node.data.X.n_obsv
        likihood_var = (model.sigma.current_value() ** 2) / n
        likihood_mean = node.data.y.summed_y() / n

        sample_value = self.hmc_sample(node, likihood_mean, likihood_var, prior_var)

        node.set_value(sample_value)
        return sample_value

    def log_posterior(self, theta_value, likihood_mean, likihood_var, prior_var, node):
        # Log prior
        log_prior = -0.5 * np.log(2 * np.pi * prior_var) - (theta_value ** 2) / (2 * prior_var)

        # Log likelihood
        n = node.data.X.n_obsv
        log_likelihood = - (n / 2) * np.log(2 * np.pi * likihood_var) - \
                         (1 / (2 * likihood_var)) * n * (likihood_mean - theta_value) ** 2

        return log_prior + log_likelihood

    def log_posterior_gradient(self, theta_value, likihood_mean, likihood_var, prior_var, node):
        gradient_prior = -theta_value / prior_var
        n = node.data.X.n_obsv
        gradient_likelihood = n / likihood_var * (likihood_mean - theta_value)
        return gradient_prior + gradient_likelihood

    def hamiltonian_dynamics(self, current_theta, current_momentum, likihood_mean, likihood_var, prior_var, node):
        new_theta = current_theta
        new_momentum = current_momentum
        for step in range(self.leapfrog_steps):
            gradient = self.log_posterior_gradient(new_theta, likihood_mean, likihood_var, prior_var, node)
            new_momentum -= 0.5 * self.step_size * gradient
            new_theta += self.step_size * new_momentum
            gradient = self.log_posterior_gradient(new_theta, likihood_mean, likihood_var, prior_var, node)
            new_momentum -= 0.5 * self.step_size * gradient

            # Break if values go beyond a reasonable range to prevent overflow
            if np.abs(new_theta) > 1e10 or np.abs(new_momentum) > 1e6:
                #print("Values are blowing up. Stopping leapfrog steps.")
                break

        return new_theta, -new_momentum

    def hmc_sample(self, node, likihood_mean, likihood_var, prior_var) -> float:
        current_theta = node.current_value
        current_momentum = np.random.normal(0, 1)

        new_theta, new_momentum = self.hamiltonian_dynamics(
            current_theta, current_momentum, likihood_mean, likihood_var, prior_var, node)
        #print('new_theta: ',new_theta)
        #print('new_momentum: ', new_momentum)
        old_hamiltonian = -self.log_posterior(current_theta, likihood_mean, likihood_var, prior_var, node)
        #print('old: ', old_hamiltonian)

        new_hamiltonian = -self.log_posterior(new_theta, likihood_mean, likihood_var, prior_var, node)
        #print('new: ', new_hamiltonian)
        u = np.random.uniform(0, 1)
        mh_ratio = np.exp(old_hamiltonian - new_hamiltonian)
        #print('mh_ratio: ',mh_ratio)

        if mh_ratio > u:
            return new_theta  # Accept new state
        else:
            return current_theta  # Stay at current state

"""## NUTS

### Sigma
"""

class SigmaSamplerNUTS(Sampler):
    def __init__(self, step_size, max_tree_depth=10, leapfrog_steps=10, epsilon=None):
        self.step_size = step_size
        self.epsilon = epsilon or step_size  # Initialize epsilon
        self.max_tree_depth = max_tree_depth
        self.total_count = 0  # Initialize total_count
        self.accepted_count = 0  # Initialize accepted_count
        self.leapfrog_steps = leapfrog_steps

    def step(self, model: Model, sigma: Sigma) -> float:
        sampled_value = self.sample(model, sigma)  # Directly call the NUTS-specific sample method
        sigma.set_value(sampled_value)
        return sampled_value

    def log_posterior(self, sigma_value, model):
        # Log prior
        posterior_alpha = model.sigma.alpha + (model.data.X.n_obsv / 2.) + 1e-5
        posterior_beta = model.sigma.beta + (0.5 * (np.sum(np.square(model.residuals())))) + 1e-5
        log_prior = posterior_alpha * np.log(posterior_beta + 1e-10) - gammaln(posterior_alpha) - \
                    (posterior_alpha + 1) * np.log(sigma_value + 1e-10) - posterior_beta / sigma_value

        # Log likelihood
        n = model.data.X.n_obsv
        residuals = model.residuals()
        log_likelihood = - (n / 2) * np.log(2 * np.pi) - (n / 2) * np.log(sigma_value ** 2) - \
                          (1 / (2 * sigma_value ** 2)) * np.sum(np.square(residuals))

        return log_prior + log_likelihood

    def log_posterior_gradient(self, sigma_value, model):
        # Gradient of log prior
        sigma_value = float(sigma_value)
        posterior_alpha = model.sigma.alpha + (model.data.X.n_obsv / 2.)
        posterior_beta = model.sigma.beta + (0.5 * (np.sum(np.square(model.residuals()))))
        log_prior_grad = - (posterior_alpha + 1) / sigma_value + posterior_beta / (sigma_value ** 2)

        # Gradient of log likelihood
        n = model.data.X.n_obsv
        residuals = model.residuals()
        log_likelihood_grad = -n / sigma_value + (1 / (sigma_value ** 3)) * np.sum(np.square(residuals))

        return log_prior_grad + log_likelihood_grad

    def hamiltonian_dynamics(self, current_sigma, current_momentum, model, epsilon):
        new_sigma = current_sigma
        new_momentum = current_momentum

        for _ in range(self.leapfrog_steps):
            # Half step update for momentum
            new_momentum -= 0.5 * self.step_size * self.log_posterior_gradient(new_sigma, model)

            # Full step update for position
            new_sigma += self.step_size * new_momentum

            # Another half step for momentum
            new_momentum -= 0.5 * self.step_size * self.log_posterior_gradient(new_sigma, model)

        return new_sigma, -new_momentum  # Reverse momentum

    def hamiltonian(self, sigma_value, momentum_value, model):
        # potential_energy = -np.log(np.clip(self.log_posterior(sigma_value, model), 1e-10, None))
        potential_energy = -self.log_posterior(sigma_value, model)
        kinetic_energy = 0.5 * momentum_value ** 2
        return potential_energy + kinetic_energy

    def build_tree(self, model, sigma, momentum, u, depth, epsilon):
        if depth == 0:
            # Base case
            new_sigma, new_momentum = self.hamiltonian_dynamics(sigma, momentum, model, epsilon)
            new_hamiltonian = self.hamiltonian(new_sigma, new_momentum, model)
            n_prime = int(u <= np.exp(self.log_posterior(new_sigma, model) - 0.5 * new_momentum ** 2))
            s_prime = int(new_hamiltonian < mp.log(u) + self.max_tree_depth)

            return new_sigma, new_momentum, new_sigma, new_sigma, n_prime, s_prime

        else:
            # Recursive case
            new_sigma, new_momentum, sigma_minus, sigma_plus, n_prime, s_prime = self.build_tree(model, sigma, momentum, u, depth - 1, epsilon)

            if s_prime == 1:
                direction = np.random.choice([-1, 1])

                if direction == -1:
                    _, _, sigma_minus, sigma_plus, n_double_prime, s_double_prime = self.build_tree(model, sigma_minus, -momentum, u, depth - 1, epsilon)
                else:
                    _, _, sigma_minus, sigma_plus, n_double_prime, s_double_prime = self.build_tree(model, sigma_plus, momentum, u, depth - 1, epsilon)

                if np.random.uniform(0, 1) < min(1, n_double_prime / n_prime):
                    new_sigma = sigma_minus if direction == -1 else sigma_plus

                s_prime = s_prime * s_double_prime
                n_prime += n_double_prime

            return new_sigma, new_momentum, sigma_minus, sigma_plus, n_prime, s_prime


    def sample(self, model: Model, sigma: Sigma) -> float:
        current_sigma = float(sigma.current_value())
        current_momentum = np.random.normal(0, 1)
        u = mp.rand() * mp.exp(self.log_posterior(current_sigma, model) - 0.5 * current_momentum ** 2)
        new_sigma, new_momentum = current_sigma, current_momentum
        sigma_minus, sigma_plus = current_sigma, current_sigma
        j, n, s = 0, 1, 1

        while s == 1:
            direction = np.random.choice([-1, 1])
            if direction == -1:
                new_sigma, new_momentum, sigma_minus, _, n_prime, s_prime = self.build_tree(model, sigma_minus, new_momentum, u, j, -self.epsilon)
            else:
                _, _, _, sigma_plus, n_prime, s_prime = self.build_tree(model, sigma_plus, new_momentum, u, j, self.epsilon)

            if s_prime == 1 and np.random.uniform(0, 1) < min(1, n_prime / n):
                new_sigma = new_sigma  # Accept new state

            n += n_prime
            s = int(s_prime and (sigma_plus - sigma_minus) * new_momentum >= 0)
            j += 1

        self.total_count += 1
        if new_sigma != current_sigma:
            self.accepted_count += 1

        return new_sigma  # Return the new state

    def acceptance_rate(self) -> float:
        if self.total_count == 0:
            return 0.0  # To avoid division by zero
        return self.accepted_count / self.total_count

class SigmaSamplerNUTS(Sampler):

    def __init__(self):
        self.total_count = 0  # Initialize total_count
        self.accepted_count = 0  # Initialize accepted_count

    def step(self, model: Model, sigma: Sigma) -> float:
        sampled_value = self.sample(model, sigma)
        sigma.set_value(sampled_value)
        self.total_count += 1  # Increment the total samples counter
        self.accepted_count += 1  # Increment the accepted samples counter
        return sampled_value

    @staticmethod
    def sample(model: Model, sigma: Sigma) -> float:
        posterior_alpha = sigma.alpha + (model.data.X.n_obsv / 2.)
        posterior_beta = sigma.beta + (0.5 * (np.sum(np.square(model.residuals()))))
        draw = np.power(np.random.gamma(posterior_alpha, 1. / posterior_beta), -0.5)
        return draw

    def acceptance_rate(self) -> float:
        if self.total_count == 0:
            return 0.0  # To avoid division by zero
        return self.accepted_count / self.total_count

"""### Leaf Node"""

class LeafNodeSamplerNUTS(Sampler):

    def __init__(self, step_size, max_tree_depth=10, leapfrog_steps=10):
        self.step_size = step_size
        self.max_tree_depth = max_tree_depth
        self.leapfrog_steps = leapfrog_steps  # Initialize leapfrog_steps here
        self.total_count = 0  # Initialize total_count
        self.accepted_count = 0  # Initialize accepted_count


    def step(self, model: Model, node) -> float:
        #print("Initial node value:", node.current_value)
        sampled_value = self.sample(model, node)
        node.set_value(sampled_value)
        return sampled_value

    def log_posterior(self, theta_value, model, node):
            # Log prior
            sigma_m = model.sigma_m
            log_prior = -0.5 * np.log(2 * np.pi * sigma_m ** 2 + 1e-10) - (theta_value ** 2 / (2 * sigma_m ** 2))

            # Log likelihood
            n = node.data.X.n_obsv
            residuals = node.data.y.summed_y() / n - theta_value
            sigma = model.sigma.current_value()
            log_likelihood = - (n / 2) * np.log(2 * np.pi * sigma ** 2) - \
                            (1 / (2 * sigma ** 2)) * np.sum(np.square(residuals))
            #print("Log Prior:", log_prior)  # Debugging line
            #print("Log Likelihood:", log_likelihood)

            return log_prior + log_likelihood

    def log_posterior_gradient(self, theta_value, model, node):
        sigma_m = model.sigma_m  # Standard deviation for the prior on theta
        sigma = model.sigma.current_value()  # Current value of the likelihood standard deviation

        n = node.data.X.n_obsv
        y_sum = node.data.y.summed_y()

        gradient_prior = -theta_value / (sigma_m ** 2)  # Gradient contribution from the prior
        gradient_likelihood = (y_sum - n * theta_value) / (sigma ** 2)  # Gradient contribution from the likelihood

        return gradient_prior + gradient_likelihood

    def hamiltonian_dynamics(self, theta, momentum, model, node, epsilon):
        new_theta = theta
        new_momentum = momentum

        # Leapfrog steps
        for _ in range(self.leapfrog_steps):  # make sure self.leapfrog_steps is initialized
            gradient = self.log_posterior_gradient(new_theta, model, node)  # Compute gradient
            new_momentum -= epsilon * gradient / 2  # Using epsilon instead of self.step_size
            new_theta += epsilon * new_momentum  # Using epsilon instead of self.step_size
            gradient = self.log_posterior_gradient(new_theta, model, node)  # Compute gradient again
            new_momentum -= epsilon * gradient / 2  # Using epsilon instead of self.step_size
            #print("New Theta in leapfrog:", new_theta)  # Debugging line
            #print("New Momentum in leapfrog:", new_momentum)  # Debugging line
        return new_theta, new_momentum  # return the updated values

    def hamiltonian(self, theta_value, momentum_value, model, node):
        # potential_energy = -np.log(np.clip(self.log_posterior(theta_value, model, node), 1e-10, None))
        potential_energy = -self.log_posterior(theta_value, model, node)
        kinetic_energy = 0.5 * momentum_value ** 2
        return potential_energy + kinetic_energy

    def build_tree(self, model, theta, momentum, u, depth, epsilon, node):
        if depth == 0:
            # Base case, single leapfrog step
            new_theta, new_momentum = self.hamiltonian_dynamics(theta, momentum, model, node, epsilon)
            new_hamiltonian = self.hamiltonian(new_theta, new_momentum, model, node)
            n_prime = int(u <= np.exp(self.log_posterior(new_theta, model, node) - 0.5 * new_momentum ** 2))
            s_prime = int(new_hamiltonian < mp.log(u) + self.max_tree_depth)
            #print("Base Case New Theta:", new_theta)
            return new_theta, new_momentum, new_theta, new_theta, n_prime, s_prime

        else:
            # Recursive case
            new_theta, new_momentum, theta_minus, theta_plus, n_prime, s_prime = self.build_tree(model, theta, momentum, u, depth - 1, epsilon, node)

            if s_prime == 1:
                direction = np.random.choice([-1, 1])

                if direction == -1:
                    _, _, theta_minus, theta_plus, n_double_prime, s_double_prime = self.build_tree(model, theta_minus, -momentum, u, depth - 1, epsilon, node)
                else:
                    _, _, theta_minus, theta_plus, n_double_prime, s_double_prime = self.build_tree(model, theta_plus, momentum, u, depth - 1, epsilon, node)

                if n_prime != 0:
                    if np.random.uniform(0, 1) < min(1, n_double_prime / n_prime):
                        new_theta = theta_minus if direction == -1 else theta_plus
                s_prime = s_prime * s_double_prime
                n_prime += n_double_prime
                #print("Recursive Case New Theta:", new_theta)

            return new_theta, new_momentum, theta_minus, theta_plus, n_prime, s_prime

    def sample(self, model: Model, node) -> float:
        # Generate initial momentum
        current_momentum = np.random.normal(0, 1)
        node.set_value(np.random.normal(0, 1))

        # Initialize
        current_theta = node.current_value
        self.total_count += 1  # Increment total samples counter
        u = mp.rand() * mp.exp(self.log_posterior(current_theta, model,node) - 0.5 * current_momentum ** 2)
        #print("Random u:", u)
        # Initialize tree
        theta_minus = theta_plus = new_theta = current_theta
        momentum_minus = momentum_plus = current_momentum
        j = 0
        n = 1
        s = 1

        while s == 1:
            # Choose a direction
            direction = np.random.choice([-1, 1])

            # Build Tree
            if direction == -1:
                theta_minus, momentum_minus, _, _, n_prime, s_prime = \
                    self.build_tree(model, theta_minus, momentum_minus, u, j, self.step_size, node)
            else:
                _, _, theta_plus, momentum_plus, n_prime, s_prime = \
                    self.build_tree(model, theta_plus, momentum_plus, u, j, self.step_size, node)

            if s_prime == 1:
                # Accept / Reject sample
                if np.random.uniform(0, 1) < min(1, n_prime / n):
                    new_theta = theta_minus if direction == -1 else theta_plus
                #print("Accepted New Theta:", new_theta)

            # Update n and s
            n += n_prime
            delta_theta = theta_plus - theta_minus
            s = int(s_prime and (delta_theta * momentum_minus >= 0) and (delta_theta * momentum_plus >= 0))

            # Increase tree depth
            j += 1

            # Stop criterion based on maximum tree depth
            if j > self.max_tree_depth:
                break

        if s_prime == 1:
            self.accepted_count += 1  # Increment accepted samples counter

        # Update node's value and return
        node.set_value(new_theta)
        return new_theta

"""# Model"""

def run_chain(model: 'BART', X: np.ndarray, y: np.ndarray):
    model.model = model._construct_model(X, y)
    return model.sampler.samples(model.model,
                                 model.n_samples,
                                 model.n_burn,
                                 model.thin,
                                 model.store_in_sample_predictions,
                                 model.store_acceptance_trace)

def delayed_run_chain():
    return run_chain

class BART(BaseEstimator, RegressorMixin):
    def __init__(self,
                 tree_sampler=get_tree_sampler(0.5, 0.5),
                 num_trees: int = 200,
                 n_chains: int = 4,
                 alpha_prior: float = 0.001,
                 beta_prior: float = 0.001,
                 num_samples: int = 2000,
                 burn_in: int = 500,
                 thin: float = 0.1,
                 alpha: float = 0.95,
                 beta: float = 2.,
                 store_in_sample_predictions: bool = False,
                 store_acceptance_trace: bool = False,
                 initializer=None,
                 n_jobs=-1,
                 max_tree_depth=12,
                 sampling_method="default",
                 leaf_nuts_leapfrog_steps: int = 15,
                 leaf_nuts_step_size: float = 1,
                 sigma_nuts_leapfrog_steps: int = 10,
                 sigma_nuts_step_size: float = 1,
                 leaf_hmc_leapfrog_steps: int = 15,
                 leaf_hmc_step_size: float = 1,
                 sigma_hmc_leapfrog_steps: int = 15,
                 sigma_hmc_step_size: float = 1,
                 nuts_max_tree_depth: int = 12
    ):
        # Original variables
        self.n_trees = num_trees
        self.n_chains = n_chains
        self.sigma_a = alpha_prior
        self.sigma_b = beta_prior
        self.n_burn = burn_in
        self.n_samples = num_samples
        self.alpha = alpha
        self.beta = beta
        self.thin = thin
        self.n_jobs = n_jobs
        self.store_in_sample_predictions = store_in_sample_predictions
        self.store_acceptance_trace = store_acceptance_trace
        self.columns = None  # Initialize columns to None
        self.tree_sampler = tree_sampler  # Initialize tree_sampler
        self.initializer = initializer  # Initialize initializer
        self.sampling_method = sampling_method  # Store the chosen method
        self.nuts_max_tree_depth = nuts_max_tree_depth  # Store max tree depth for NUTS

        # Initialize HMC and NUTS samplers based on provided parameters
        leaf_sampler_hmc = LeafNodeSamplerHMC(leapfrog_steps=leaf_hmc_leapfrog_steps, step_size=leaf_hmc_step_size)
        sigma_sampler_hmc = SigmaSamplerHMC(leapfrog_steps=sigma_hmc_leapfrog_steps, step_size=sigma_hmc_step_size)

        leaf_sampler_nuts = LeafNodeSamplerNUTS(step_size=leaf_nuts_step_size,
                                                 max_tree_depth=nuts_max_tree_depth,
                                                 leapfrog_steps=leaf_nuts_leapfrog_steps)
        sigma_sampler_nuts = SigmaSamplerNUTS(#step_size=sigma_nuts_step_size,
                                              #max_tree_depth=nuts_max_tree_depth
        )

        # Initialize Adaptive Metropolis samplers
        leaf_sampler_adap = AdaptiveLeafNodeSampler(init_var_leaf=1.0, adapt_window=50)
        sigma_sampler_adap = AdaptiveSigmaSampler(init_var_sigma=1.0, adapt_window=50)

        # Initialize Sample Schedules for default, HMC and NUTS
        self.sample_schedule_basic = SampleSchedule(tree_sampler=self.tree_sampler, leaf_sampler=LeafNodeSampler(), sigma_sampler=SigmaSampler())
        self.sample_schedule_adap = SampleSchedule(tree_sampler=self.tree_sampler, leaf_sampler=leaf_sampler_adap, sigma_sampler=sigma_sampler_adap)

        self.sample_schedule_hmc = SampleSchedule(tree_sampler=self.tree_sampler, leaf_sampler=leaf_sampler_hmc, sigma_sampler=sigma_sampler_hmc)
        self.sample_schedule_nuts = SampleSchedule(tree_sampler=self.tree_sampler, leaf_sampler=leaf_sampler_nuts, sigma_sampler=sigma_sampler_nuts)

        # Set the sampler based on the chosen method
        if self.sampling_method == 'hmc':
            self.sampler = ModelSampler(self.sample_schedule_hmc)
        elif self.sampling_method == 'nuts':
            self.sampler = ModelSampler(self.sample_schedule_nuts)
        elif self.sampling_method == 'adaptive':
            self.sampler = ModelSampler(self.sample_schedule_adap)
        elif self.sampling_method == 'traditional':
            self.sampler = ModelSampler(self.sample_schedule_basic)
        else:
            print('Raise Error: Sampling Method NOT Exist')


    def fit(self, X, y) -> 'BART':
        self.model = self._construct_model(X, y)
        self.extract = Parallel(n_jobs=1)(self.f_delayed_chains(X, y))
        self.combined_chains = self._combine_chains(self.extract)
        self._model_samples, self._prediction_samples = self.combined_chains["model"], self.combined_chains[
            "in_sample_predictions"]
        return self

    @staticmethod
    def _combine_chains(extract):
        keys = list(extract[0].keys())
        combined = {}
        for key in keys:
            combined[key] = np.concatenate([chain[key] for chain in extract], axis=0)
        return combined

    @staticmethod
    def _convert_covariates_to_data(X: np.ndarray, y: np.ndarray) -> Data:
        from copy import deepcopy
        if type(X) == pd.DataFrame:
            X: pd.DataFrame = X
            X = X.values
        return Data(deepcopy(X), deepcopy(y), normalize=True)

    def _construct_model(self, X: np.ndarray, y: np.ndarray) -> Model:
        self.data = self._convert_covariates_to_data(X, y)
        self.sigma = Sigma(self.sigma_a, self.sigma_b, self.data.y.normalizing_scale)
        self.model = Model(self.data,
                           self.sigma,
                           n_trees=self.n_trees,
                           alpha=self.alpha,
                           beta=self.beta,
                           initializer=self.initializer)
        return self.model

    def f_delayed_chains(self, X: np.ndarray, y: np.ndarray):
        return [delayed(x)(self, X, y) for x in self.f_chains()]

    def f_chains(self):

        return [delayed_run_chain() for _ in range(self.n_chains)]

    def predict(self, X) -> np.ndarray:
        return self._out_of_sample_predict(X)

    def _out_of_sample_predict(self, X):
        return self.data.y.unnormalize_y(np.mean([x.predict(X) for x in self._model_samples], axis=0))

    def fit_predict(self, X, y):
        self.fit(X, y)
        return self.predict(X)

    @property
    def model_samples(self) -> List[Model]:
        return self._model_samples

"""# Evaluation Metrics"""

from typing import List, Tuple
import matplotlib.pyplot as plt
import numpy as np

# Function to extract sigma samples from BART model
def extract_sigma_samples(model_samples: List) -> np.ndarray:
    return np.array([model.sigma.current_value() for model in model_samples])

# Function to extract leaf node values for each tree in BART model
def extract_leaf_node_samples(model_samples: List) -> List[np.ndarray]:
    leaf_samples_per_tree = []
    for model in model_samples:
        for tree in model.trees:
            leaf_values = np.array([leaf.current_value for leaf in tree.leaf_nodes])
            leaf_samples_per_tree.append(leaf_values)
    return leaf_samples_per_tree

# Function to plot trace of samples
def plot_trace(samples: List[np.ndarray], param_name: str):
    plt.figure(figsize=(10, 4))
    for i, sample in enumerate(samples):
        plt.plot(sample, label=f'Tree {i + 1}')
    plt.title(f'Trace for {param_name}')
    plt.xlabel('Iteration')
    plt.ylabel(param_name)
    plt.legend()
    plt.show()

# Function to plot autocorrelation
def plot_autocorrelation(samples: List[np.ndarray], param_name: str):
    plt.figure(figsize=(10, 4))
    for i, sample in enumerate(samples):
        max_lag = 50
        acf = np.correlate(sample - np.mean(sample), sample - np.mean(sample), mode='full')
        acf = acf[int(acf.size / 2):]
        acf /= np.max(acf)
        plt.plot(acf[:max_lag], label=f'Tree {i + 1}')
    plt.title(f'Autocorrelation for {param_name}')
    plt.xlabel('Lag')
    plt.ylabel('Autocorrelation')
    plt.legend()
    plt.show()

def compute_gelman_rubin(samples: List[np.ndarray]) -> float:
    m = len(samples)  # Number of chains

    if m < 2:
        raise ValueError("Gelman-Rubin diagnostic requires multiple chains. Only one chain provided.")

    # Make sure all chains have the same length and shape
    n = len(samples[0])  # Length of first chain
    first_shape = np.shape(samples[0][0])  # Shape of the first sample
    for chain in samples:
        if len(chain) != n:
            raise ValueError("All chains must have the same length.")
        for sample in chain:
            if np.shape(sample) != first_shape:
                raise ValueError("All samples must have the same shape.")

    # If checks passed, then proceed to flatten and calculate the statistic
    flat_samples = [np.array([sample.flatten() for sample in chain]) for chain in samples]

    # Compute within-chain variance
    W = np.mean([np.var(chain, axis=0, ddof=1) for chain in flat_samples], axis=0)

    # Compute between-chain variance
    overall_mean = np.mean(np.concatenate(flat_samples, axis=0), axis=0)
    B = n / (m - 1) * np.sum([(np.mean(chain, axis=0) - overall_mean)**2 for chain in flat_samples], axis=0)

    # Compute Gelman-Rubin statistic
    R_hat = (1 - 1/n + B/W) / (1 - 1/n)

    return R_hat


def effective_sample_size(samples):
    n = len(samples)
    mean = np.mean(samples)
    var = np.var(samples, ddof=1)
    acf = np.correlate(samples - mean, samples - mean, mode='full') / var / n
    acf = acf[int(n - 1):]

    # Sum up the autocorrelations until they go out of significance
    ess = n / (1 + 2 * np.sum(acf[1:]))
    return ess

def tail_variance(samples, tail_length=20):
    # Extract the tail of the chain
    tail_samples = samples[-tail_length:]

    # Compute the variance of the tail
    tail_var = np.var(tail_samples)

    return tail_var

"""# CI Implementation

### Tradtional
"""

num_trees=50
bart_model = BART(num_trees=num_trees,
                  alpha=0.95,
                  beta=2,
                  burn_in=20,
                  num_samples=600,
                  alpha_prior=alpha_prior,
                  beta_prior=beta_prior,
                  sampling_method='traditional', # Activation Parameter
                  max_tree_depth=10)
stored_trees = bart_model.fit(X_full, Y)

MSE_Traditional = np.sum((bart_model.predict(X_full)-Y)**2)
MSE_Traditional, MSE_Traditional**0.5

y_control_pred = bart_model.predict(X_control)
y_treated_pred = bart_model.predict(X_treated)
ITEs = y_treated_pred - y_control_pred
ATE = np.mean(ITEs)
print(f"Estimated Average Treatment Effect (ATE): {ATE}")

sigma_samples = extract_sigma_samples(bart_model.model_samples)
leaf_samples = extract_leaf_node_samples(bart_model.model_samples)
plot_trace([sigma_samples], 'Sigma')
plot_autocorrelation([sigma_samples], 'Sigma')

ess = effective_sample_size(sigma_samples)
print(f"Effective Sample Size for Sigma: {ess}")



tail_variance(sigma_samples, 20)**0.5

leaf_samples=np.array(leaf_samples)
leaf_samples = leaf_samples.squeeze()
leaf_samples = leaf_samples[::num_trees]

type(leaf_samples)

leaf_samples.shape

plot_trace([leaf_samples], "Leaf Node Values")
# plot_autocorrelation(leaf_samples[0], "Leaf Node Values")

ess = effective_sample_size(leaf_samples[:,0])
print(f"Effective Sample Size for Sigma: {ess}")

plot_autocorrelation([leaf_samples], "Leaf Node Values")

n_chains = 3
all_sigma_samples = []
all_leaf_node_samples = []
for _ in range(n_chains):
    bart_model = BART(num_trees=20,
                      alpha=alpha,
                      beta=beta,
                      burn_in=20,
                      num_samples=100,
                      alpha_prior=alpha_prior,
                      beta_prior=beta_prior,
                      sampling_method='traditional')
    bart_model.fit(X_full, Y)
    sigma_samples = extract_sigma_samples(bart_model.model_samples)
    leaf_node_samples = extract_leaf_node_samples(bart_model.model_samples)
    all_sigma_samples.append(sigma_samples)
    all_leaf_node_samples.append(leaf_node_samples)

print(f"Gelman-Rubin Diagnostic (R-hat) for Sigma: {compute_gelman_rubin(all_sigma_samples)}")

print(f"Gelman-Rubin Diagnostic (R-hat) for Sigma: {compute_gelman_rubin(all_leaf_node_samples)}")

"""### hmc"""

num_trees=50

bart_model_hmc = BART(num_trees=num_trees,
                  alpha=0.95,
                  beta=2,
                  burn_in=20,
                  num_samples=600,
                  alpha_prior=alpha_prior,
                  beta_prior=beta_prior,
                  sampling_method='hmc', # Activation Parameter
                  leaf_hmc_step_size=0.000001,
                  sigma_hmc_step_size=0.00001,
                  max_tree_depth=20,
                  leaf_hmc_leapfrog_steps = 5,
                  sigma_hmc_leapfrog_steps = 20)
stored_trees_hmc = bart_model_hmc.fit(X_full, Y)

y_control_pred = bart_model_hmc.predict(X_control)
y_treated_pred = bart_model_hmc.predict(X_treated)
ITEs = y_treated_pred - y_control_pred
ATE = np.mean(ITEs)
print(f"Estimated Average Treatment Effect (ATE): {ATE}")

MSE_HMC = np.sum((bart_model_hmc.predict(X_full)-Y)**2)
MSE_HMC-MSE_Traditional, MSE_HMC**0.5

sigma_samples_hmc = extract_sigma_samples(bart_model_hmc.model_samples)
leaf_samples_hmc = extract_leaf_node_samples(bart_model_hmc.model_samples)
plot_trace([sigma_samples_hmc], 'Sigma')
plot_autocorrelation([sigma_samples_hmc], 'Sigma')

ess = effective_sample_size(sigma_samples_hmc)
print(f"Effective Sample Size for Sigma: {ess}")

tail_variance(sigma_samples_hmc, 20)**0.5

def extract_first_leaf_node_samples(model_samples: List) -> List[np.ndarray]:
    first_leaf_samples_per_tree = []
    for model in model_samples:
        for tree in model.trees:
            first_leaf_value = tree.leaf_nodes[0].current_value  # Assuming leaf_nodes is a list and we want the first
            first_leaf_samples_per_tree.append(first_leaf_value)
    return first_leaf_samples_per_tree

leaf_samples_hmc = extract_first_leaf_node_samples(bart_model_hmc.model_samples)

leaf_samples_hmc = np.array(leaf_samples_hmc)
leaf_samples_hmc = leaf_samples_hmc.squeeze()
leaf_samples_hmc = leaf_samples_hmc[::num_trees]

plot_trace([leaf_samples_hmc], "Leaf Node Values")
plot_autocorrelation([leaf_samples_hmc], "Leaf Node Values")

ess = effective_sample_size(leaf_samples_hmc)
print(f"Effective Sample Size for Leaf Nodes: {ess}")

n_chains = 3
all_sigma_samples = []
all_leaf_node_samples = []
for _ in range(n_chains):
    bart_model = BART(num_trees=20,
                      alpha=alpha,
                      beta=beta,
                      burn_in=20,
                      num_samples=200,
                      alpha_prior=alpha_prior,
                      beta_prior=beta_prior,
                      sampling_method='hmc',
                      leaf_hmc_step_size=0.000001,
                  sigma_hmc_step_size=0.00001,
                  max_tree_depth=20,
                  leaf_hmc_leapfrog_steps = 5,
                  sigma_hmc_leapfrog_steps = 20)
    bart_model.fit(X_full, Y)
    sigma_samples = extract_sigma_samples(bart_model.model_samples)
    leaf_node_samples = extract_leaf_node_samples(bart_model.model_samples)
    all_sigma_samples.append(sigma_samples)
    all_leaf_node_samples.append(leaf_node_samples)

print(f"Gelman-Rubin Diagnostic (R-hat) for Sigma: {compute_gelman_rubin(all_sigma_samples)}")

print(f"Gelman-Rubin Diagnostic (R-hat) for Sigma: {compute_gelman_rubin(all_leaf_node_samples)}")

"""### NUTS"""

num_trees = 20
bart_model_nuts = BART(num_trees=num_trees,
                  alpha=0.95,
                  beta=2,
                  burn_in=20,
                  num_samples=600,
                  alpha_prior=2,
                  beta_prior=2,
                  sampling_method='nuts', # Activation Parameter
                  leaf_nuts_step_size=0.00001,
                  #sigma_nuts_step_size=10,
                  max_tree_depth=20)
stored_trees = bart_model_nuts.fit(X_full, Y)

num_trees = 20

MSE_NUTS = np.sum((bart_model_nuts.predict(X_full)-Y)**2)
MSE_NUTS**0.5

y_control_pred = bart_model_nuts.predict(X_control)
y_treated_pred = bart_model_nuts.predict(X_treated)
ITEs = y_treated_pred - y_control_pred
ATE = np.mean(ITEs)
print(f"Estimated Average Treatment Effect (ATE): {ATE}")

sigma_samples_nuts = extract_sigma_samples(bart_model_nuts.model_samples)
leaf_samples_nuts = extract_leaf_node_samples(bart_model_nuts.model_samples)
plot_trace([sigma_samples_nuts], 'Sigma')
plot_autocorrelation([sigma_samples_nuts], 'Sigma')

leaf_samples_nuts = np.array(leaf_samples_nuts)
leaf_samples_nuts.shape

leaf_samples_nuts = leaf_samples_nuts[::num_trees]
leaf_samples_nuts.shape

leaf_samples_nuts

leaf_samples_nuts = leaf_samples_nuts.flatten()

leaf_samples_nuts = leaf_samples_nuts.reshape(-1,1)

leaf_samples_nuts = np.vstack([arr.reshape(-1, 1) for arr in leaf_samples_nuts])

leaf_samples_nuts.shape,type(leaf_samples_nuts)

plot_trace([leaf_samples_nuts], "Leaf Node Values")

leaf_samples_nuts = leaf_samples_nuts.reshape(-1,1)

leaf_samples_nuts.shape



n_chains = 3
all_sigma_samples = []
all_leaf_node_samples = []
for _ in range(n_chains):
    bart_model = BART(num_trees=10,
                      alpha=alpha,
                      beta=beta,
                      burn_in=20,
                      num_samples=200,
                      alpha_prior=alpha_prior,
                      beta_prior=beta_prior,
                      sampling_method='nuts',
                      leaf_hmc_step_size=0.0001,
                  #sigma_hmc_step_size=0.00001,
                  max_tree_depth=20,
                  leaf_hmc_leapfrog_steps = 5,
                  sigma_hmc_leapfrog_steps = 20)
    bart_model.fit(X_full, Y)
    sigma_samples = extract_sigma_samples(bart_model.model_samples)
    leaf_node_samples = extract_leaf_node_samples(bart_model.model_samples)
    all_sigma_samples.append(sigma_samples)
    all_leaf_node_samples.append(leaf_node_samples)

print(f"Gelman-Rubin Diagnostic (R-hat) for Sigma: {compute_gelman_rubin(all_sigma_samples)}")

all_sigma_samples = np.array(all_sigma_samples)
all_sigma_samples.shape

leaf_samples_nuts = np.array(all_leaf_node_samples)
leaf_samples_nuts = leaf_samples_nuts.squeeze()
leaf_samples_nuts = leaf_samples_nuts[::num_trees]

leaf_samples_nuts.shape

leaf_samples_nuts.shape

print(f"Gelman-Rubin Diagnostic (R-hat) for Sigma: {compute_gelman_rubin(all_leaf_node_samples)}")

ess = effective_sample_size(sigma_samples_nuts)
print(f"Effective Sample Size for Sigma: {ess}")
print(f"Relative Effective Sample Size for Sigma: {ess}")

ess = effective_sample_size(leaf_samples_nuts)
print(f"Effective Sample Size for Leaf Nodes: {ess}")

"""## Adaptive"""

num_trees = 50

bart_model_adap = BART(num_trees=num_trees,
                  alpha=0.95,
                  beta=2,
                  burn_in=50,
                  num_samples=600,
                  alpha_prior=2,
                  beta_prior=2,
                  sampling_method='adaptive', # Activation Parameter
                  max_tree_depth=10
                  )
stored_trees = bart_model_adap.fit(X_full, Y)

sigma_samples_adap = extract_sigma_samples(bart_model_adap.model_samples)
leaf_samples_adap = extract_leaf_node_samples(bart_model_adap.model_samples)
plot_trace([sigma_samples_adap], 'Sigma')
plot_autocorrelation([sigma_samples_adap], 'Sigma')

leaf_samples_adap = np.array(leaf_samples_adap)
leaf_samples_adap = leaf_samples_adap.squeeze()
leaf_samples_adap = leaf_samples_adap[::num_trees]

leaf_samples_adap = leaf_samples_adap[:,0]

plot_trace([leaf_samples_adap], "Leaf Node Values")

plot_autocorrelation([leaf_samples_adap], "Leaf Node Values")

ESS = effective_sample_size(leaf_samples_adap)
print(f"Effective Sample Size for Leaf Nodes: {ESS}")

ESS = effective_sample_size(sigma_samples_adap)
print(f"Effective Sample Size for Sigma: {ESS}")

n_chains = 4
all_sigma_samples = []
all_leaf_node_samples = []
for _ in range(n_chains):
    bart_model = BART(num_trees=15,
                      alpha=alpha,
                      beta=beta,
                      burn_in=20,
                      num_samples=250,
                      alpha_prior=alpha_prior,
                      beta_prior=beta_prior,
                      sampling_method='adaptive',
                  max_tree_depth=20,)
    bart_model.fit(X_full, Y)
    sigma_samples = extract_sigma_samples(bart_model.model_samples)
    leaf_node_samples = extract_leaf_node_samples(bart_model.model_samples)
    all_sigma_samples.append(sigma_samples)
    all_leaf_node_samples.append(leaf_node_samples)

print(f"Gelman-Rubin Diagnostic (R-hat) for Sigma: {compute_gelman_rubin(all_sigma_samples)}")

print(f"Gelman-Rubin Diagnostic (R-hat) for Sigma: {compute_gelman_rubin(all_leaf_node_samples)}")

MSE_adap = np.sum((bart_model_adap.predict(X_full)-Y)**2)
MSE_adap**0.5





"""## Application of best model"""

from causalml.inference.meta import LRSRegressor
lr = LRSRegressor()
ate = lr.estimate_ate(X, treatment, y)
print(ate)